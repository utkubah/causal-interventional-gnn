{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5fea15d",
   "metadata": {},
   "source": [
    "# Synthetic SCM: Generation, Interventions, and ATE Benchmarks\n",
    "\n",
    "**Goal.** Create a controlled 7-node synthetic dataset with the **same topology** as the finance DAG. We will:\n",
    "- define **structural equations** with noise,\n",
    "- generate observational samples,\n",
    "- generate **true interventional** samples with `do(·)`,\n",
    "- compute ground-truth **ATE**s per edge for benchmarking models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d3ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "# PyTorch Geometric imports\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "# CONFIGURATION & PATHS\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))  \n",
    "\n",
    "\n",
    "CONFIGS_DIR = PROJECT_ROOT / \"configs\"\n",
    "\n",
    "CFG_PATH = CONFIGS_DIR / \"best_config.yaml\"\n",
    "with open(CFG_PATH, \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "seed = int(cfg.get(\"seed\", 42))\n",
    "random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "from src.models import GNN_NCM, BaselineGCN\n",
    "from src.trainer import CausalTwoPartTrainer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a967666",
   "metadata": {},
   "source": [
    "## 1. Structural Equations\n",
    "\n",
    "Use simple nonlinearities to mimic saturation in microstructure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4b7e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define graph structure\n",
    "nodes = [\"Mom\", \"HML\", \"OI\", \"PC\", \"BAS\", \"LIQ\", \"VOL\"]\n",
    "node_mapping = {name: i for i, name in enumerate(nodes)}\n",
    "edges = [\n",
    "    ('Mom', 'HML'), ('Mom', 'PC'), ('HML', 'OI'), ('OI', 'PC'),\n",
    "    ('OI', 'BAS'), ('PC', 'BAS'), ('BAS', 'LIQ'), ('BAS', 'VOL')\n",
    "]\n",
    "edge_index = torch.tensor([[node_mapping[src], node_mapping[dst]] for src, dst in edges], dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Define ground-truth causal laws\n",
    "causal_relationships = {\n",
    "    'HML': {'Mom': 0.5}, 'OI': {'HML': 0.7},\n",
    "    'PC':  {'Mom': 0.3, 'OI': 0.6},\n",
    "    'BAS': {'OI': 0.4, 'PC': 0.6}, # This will be non-linear (tanh)\n",
    "    'LIQ': {'BAS': -0.8}, 'VOL': {'BAS': 0.9}\n",
    "}\n",
    "NOISE_STD = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2bd8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticGraphDataset(Dataset):\n",
    "    def __init__(self, num_graphs=50, num_nodes=7, feature_dim=4,\n",
    "                 edge_index=None, causal_relationships=None,\n",
    "                 node_mapping=None, noise_std=0.1, seed=0):\n",
    "        super().__init__()\n",
    "        self.num_graphs = num_graphs\n",
    "        self.num_nodes = num_nodes\n",
    "        self.feature_dim = feature_dim  \n",
    "        self.edge_index = edge_index\n",
    "        self.causal_relationships = causal_relationships\n",
    "        self.node_mapping = node_mapping\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        g = torch.Generator().manual_seed(seed)\n",
    "        self._xs = torch.randn(num_graphs, num_nodes, feature_dim, generator=g)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_graphs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self._xs[idx].clone()\n",
    "        y = torch.zeros(self.num_nodes, 1)\n",
    "\n",
    "        for node_name, parents in self.causal_relationships.items():\n",
    "            child_idx = self.node_mapping[node_name]\n",
    "            parent_effects = 0.0\n",
    "            for parent_name, coeff in parents.items():\n",
    "                parent_idx = self.node_mapping[parent_name]\n",
    "                parent_effects += x[parent_idx].mean() * coeff\n",
    "            if node_name == 'BAS':\n",
    "                parent_effects = torch.tanh(parent_effects)\n",
    "            y[child_idx] = parent_effects + torch.randn(1) * self.noise_std\n",
    "\n",
    "        return Data(x=x, edge_index=self.edge_index, y=y)\n",
    "\n",
    "\n",
    "ds = SyntheticGraphDataset(\n",
    "    num_graphs=100,\n",
    "    num_nodes=7,\n",
    "    feature_dim=1,\n",
    "    edge_index=edge_index,\n",
    "    causal_relationships=causal_relationships,\n",
    "    node_mapping=node_mapping,\n",
    "    noise_std=NOISE_STD,\n",
    "    seed=123,\n",
    ")\n",
    "\n",
    "print(len(ds))\n",
    "print(ds[0])  # first synthetic graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52766a43",
   "metadata": {},
   "source": [
    "## 2. Training and Evaluating on Observational Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54151542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "mcfg = cfg.get(\"model\", {})\n",
    "best_params = {\n",
    "    \"hidden_dim\": int(mcfg.get(\"hidden_dim\", 32)),\n",
    "    \"out_dim\":    int(mcfg.get(\"out_dim\", 16)),\n",
    "    \"noise_dim\":  int(mcfg.get(\"noise_dim\", 4)),\n",
    "}\n",
    "\n",
    "\n",
    "tcfg = cfg.get(\"training\", {})\n",
    "trainer_kwargs = dict(\n",
    "    epochs_obs   = int(tcfg.get(\"epochs_obs\", 40)),\n",
    "    epochs_do    = int(tcfg.get(\"epochs_do\", 20)),\n",
    "    lr           = float(tcfg.get(\"lr\", 1e-2)),\n",
    "    w_obs        = float(tcfg.get(\"w_obs\", 0.2)),\n",
    "    w_do         = float(tcfg.get(\"w_do\", 1.0)),\n",
    "    weight_decay = float(tcfg.get(\"weight_decay\", 1e-4)),\n",
    "    clip         = float(tcfg.get(\"clip\", 1.0)),\n",
    "    neutral      = tcfg.get(\"neutral\", \"zeros\"),\n",
    "    delta        = float(tcfg.get(\"delta\", 0.1)),\n",
    ")\n",
    "\n",
    "n_train = int(0.8 * len(ds))\n",
    "n_val   = len(ds) - n_train\n",
    "ds_train, ds_val = random_split(ds, [n_train, n_val], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(ds_train, batch_size=1, shuffle=True)  # keep 1 if graphs vary\n",
    "val_loader   = DataLoader(ds_val,   batch_size=1, shuffle=False)\n",
    "\n",
    "models = {}\n",
    "g0 = next(iter(train_loader))\n",
    "num_edges = g0.edge_index.size(1)\n",
    "num_features = g0.num_features\n",
    "num_edges    = g0.edge_index.size(1)\n",
    "device = g0.x.device\n",
    "\n",
    "\n",
    "# --- Train all three models ---\n",
    "\n",
    "# 1. GNN-NCM (per-edge)\n",
    "print(\"\\n--- Training GNN-NCM (per-edge) ---\")\n",
    "model_per_edge = GNN_NCM(num_features=num_features, num_edges=num_edges, gnn_mode='per_edge', **best_params)\n",
    "trainer_per_edge = CausalTwoPartTrainer(**trainer_kwargs)\n",
    "\n",
    "trainer_per_edge.train(model_per_edge, train_loader, val_loader=val_loader)\n",
    "val_loss_pe = trainer_per_edge.evaluate_obs_mse(model_per_edge, val_loader)\n",
    "print(f\"[per-edge] val_mse={val_loss_pe:.6f}\")\n",
    "models['GNN-NCM (per-edge)'] = model_per_edge\n",
    "\n",
    "# 2. GNN-NCM (shared) - Ablation \n",
    "print(\"\\n--- Training GNN-NCM (shared) ---\")\n",
    "model_shared = GNN_NCM(num_features=num_features, num_edges=num_edges, gnn_mode='shared', **best_params)\n",
    "trainer_shared = CausalTwoPartTrainer(**trainer_kwargs)\n",
    "\n",
    "trainer_shared.train(model_shared, train_loader, val_loader=val_loader)\n",
    "val_loss_pe = trainer_shared.evaluate_obs_mse(model_shared, val_loader)\n",
    "print(f\"[shared] val_mse={val_loss_pe:.6f}\")\n",
    "models['GNN-NCM (shared)'] = model_shared\n",
    "\n",
    "\n",
    "# 3. Baseline GCN - A standard, non-causal GNN\n",
    "print(\"\\n--- Training BaselineGCN ---\")\n",
    "baseline_model = BaselineGCN(num_features=num_features, hidden_dim=best_params['hidden_dim'], out_dim=best_params['out_dim'])\n",
    "optimizer_baseline = optim.Adam(baseline_model.parameters())\n",
    "loss_fn_baseline = nn.MSELoss()\n",
    "\n",
    "for ep in range(200):\n",
    "    for g in train_loader:\n",
    "        g = g.to(device)\n",
    "        baseline_model.train(); optimizer_baseline.zero_grad()\n",
    "        pred = baseline_model(g.x, g.edge_index)\n",
    "        loss = loss_fn_baseline(pred, g.y)\n",
    "        loss.backward(); optimizer_baseline.step()\n",
    "    if (ep+1) % 40 == 0:\n",
    "        print(f\"[Baseline] ep {ep+1:03d} loss={loss.item():.4f}\")\n",
    "\n",
    "# quick baseline val\n",
    "baseline_model.eval()\n",
    "with torch.no_grad():\n",
    "    total = 0.0; n = 0\n",
    "    for g in val_loader:\n",
    "        g = g.to(device)\n",
    "        total += float(((baseline_model(g.x, g.edge_index) - g.y)**2).mean().item())\n",
    "        n += 1\n",
    "val_loss_bl = total / max(n,1)\n",
    "print(f\"[baseline] val_mse={val_loss_bl:.6f}\")\n",
    "\n",
    "\n",
    "models['BaselineGCN'] = baseline_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeade3f",
   "metadata": {},
   "source": [
    "### 2.1 Analysis of Causal Fidelity (ATE Recovery)\n",
    "\n",
    "This is the ultimate test of our model. We will calculate the **true** Average Treatment Effect (ATE) of an intervention directly from our synthetic world's rules. Then, we will ask each model to **estimate** the ATE using its learned mechanisms. The model that gets closest to the true ATE is the one that has best learned the underlying causal structure.\n",
    "\n",
    "We will test the ATE of `do(BAS = BAS + 1)` on `VOL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3536b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    device = g0.x.device\n",
    "    bas_idx = node_mapping['BAS']\n",
    "    y0 = models['GNN-NCM (per-edge)'](g0.x, g0.edge_index)\n",
    "    y1 = models['GNN-NCM (per-edge)'].do_intervention(\n",
    "        g0.x, g0.edge_index,\n",
    "        intervened_nodes=torch.tensor([bas_idx], dtype=torch.long, device=device),\n",
    "        new_feature_values=(g0.x[bas_idx] + 1.0).unsqueeze(0)\n",
    "    )\n",
    "    diff = (y1 - y0).abs().sum().item()\n",
    "    print(\"Δ after do(BAS+=1):\", diff)  # if this prints 0.0, it means model ignores the intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32e4600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple ATE on BAS (target: VOL)\n",
    "import torch, pandas as pd, matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = g0.x.device\n",
    "bas_idx = node_mapping['BAS']\n",
    "vol_idx = node_mapping['VOL']\n",
    "delta = 1.0\n",
    "\n",
    "def mech_eval(x):\n",
    "    y = torch.zeros_like(g0.y)\n",
    "    for child, parents in causal_relationships.items():\n",
    "        ci = node_mapping[child]\n",
    "        s = 0.0\n",
    "        for p, w in parents.items():\n",
    "            pi = node_mapping[p]\n",
    "            s += x[pi].mean() * w\n",
    "        if child == 'BAS':\n",
    "            s = torch.tanh(s)\n",
    "        y[ci] = s\n",
    "    return y\n",
    "\n",
    "# true ATE (noise-free)\n",
    "x_before = g0.x.clone()\n",
    "x_after  = x_before.clone(); x_after[bas_idx, :] = x_after[bas_idx, :] + delta\n",
    "y_before_true = mech_eval(x_before)\n",
    "y_after_true  = mech_eval(x_after)\n",
    "true_ate = y_after_true - y_before_true\n",
    "\n",
    "rows = []\n",
    "for name, model in models.items():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds_before = model(g0.x, g0.edge_index)\n",
    "        if \"GNN-NCM\" in name:\n",
    "            preds_after = model.do_intervention(\n",
    "                g0.x, g0.edge_index,\n",
    "                intervened_nodes=torch.tensor([bas_idx], dtype=torch.long, device=device),\n",
    "                new_feature_values=(g0.x[bas_idx] + delta).unsqueeze(0)\n",
    "            )\n",
    "        else:\n",
    "            x_obs = g0.x.clone()\n",
    "            x_obs[bas_idx, :] = x_obs[bas_idx, :] + delta\n",
    "            preds_after = model(x_obs, g0.edge_index)\n",
    "        est_ate = preds_after - preds_before\n",
    "\n",
    "    rows.append({\n",
    "        'model': name,\n",
    "        'Estimated ATE (VOL)': est_ate[vol_idx].item(),\n",
    "        'True ATE (VOL)': true_ate[vol_idx].item()\n",
    "    })\n",
    "\n",
    "ate_df = pd.DataFrame(rows).set_index('model')\n",
    "print(\"\\n--- ATE Recovery (do(BAS = +1.0) → VOL) ---\")\n",
    "print(ate_df)\n",
    "\n",
    "ax = ate_df.plot(kind='bar', rot=0, figsize=(10,6),\n",
    "                 title=\"ATE Recovery for do(BAS = +1.0) on VOL\")\n",
    "ax.grid(axis='y', linestyle='--')\n",
    "plt.ylabel(\"Average Treatment Effect (ATE)\")\n",
    "plt.xlabel(\"Model Type\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b401614",
   "metadata": {},
   "source": [
    "## 3. Interventional Data\n",
    "\n",
    "Pick a parent \\(p\\), **replace** its equation by \\(p:=x^\\*\\), keep other mechanisms unchanged (graph surgery), then sample downstream. Then train on interventional dataset using interventional supervised learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef705a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "import torch\n",
    "\n",
    "def build_sem_mats(causal_relationships, node_mapping, device, dtype):\n",
    "    N = len(node_mapping)\n",
    "    W = torch.zeros(N, N, device=device, dtype=dtype)\n",
    "    for child, parents in causal_relationships.items():\n",
    "        ci = node_mapping[child]\n",
    "        for p, w in parents.items():\n",
    "            W[ci, node_mapping[p]] = float(w)\n",
    "    bas_idx = node_mapping.get('BAS', None)  # nonlinearity node\n",
    "    return W, bas_idx\n",
    "\n",
    "def sem_eval_vec(v, W, bas_idx=None):\n",
    "    y = v @ W.T if v.ndim == 2 else W @ v\n",
    "    if bas_idx is not None:\n",
    "        if y.ndim == 1: y[bas_idx] = torch.tanh(y[bas_idx])\n",
    "        else:           y[:, bas_idx] = torch.tanh(y[:, bas_idx])\n",
    "    return y.unsqueeze(-1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4108e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InterventionalDataset\n",
    "from torch_geometric.data import Dataset, Data\n",
    "\n",
    "class InterventionalDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 base_dataset,\n",
    "                 node_mapping,\n",
    "                 causal_relationships,\n",
    "                 deltas=(+1.0, -1.0),\n",
    "                 per_graph_per_node=2,\n",
    "                 transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root=None, transform=transform,\n",
    "                         pre_transform=pre_transform, pre_filter=pre_filter)\n",
    "        self.base = base_dataset\n",
    "        self.node_mapping = node_mapping\n",
    "        self.deltas = list(deltas)\n",
    "        self.per_graph_per_node = int(per_graph_per_node)\n",
    "\n",
    "        s0 = self.base[0]\n",
    "        self.W, self.bas_idx = build_sem_mats(causal_relationships, node_mapping,\n",
    "                                              device=s0.x.device, dtype=s0.x.dtype)\n",
    "        self._v = []\n",
    "        for gi in range(len(self.base)):\n",
    "            g = self.base[gi]\n",
    "            self._v.append(g.x.mean(dim=1))  # [N]\n",
    "\n",
    "        self._index = []\n",
    "        for gi in range(len(self.base)):\n",
    "            for _, nidx in node_mapping.items():\n",
    "                for k in range(self.per_graph_per_node):\n",
    "                    self._index.append((gi, int(nidx), float(self.deltas[k % len(self.deltas)])))\n",
    "\n",
    "\n",
    "        self._cache = {} \n",
    "\n",
    "    def len(self): return len(self._index)\n",
    "    def __len__(self): return self.len()\n",
    "\n",
    "    def get(self, i):\n",
    "        gi, nidx, delta = self._index[i]\n",
    "        g = self.base[gi]\n",
    "        key = (gi, nidx, delta)\n",
    "        if key not in self._cache:\n",
    "            v = self._v[gi].clone()\n",
    "            v[nidx] = v[nidx] + delta\n",
    "            y_do = sem_eval_vec(v, self.W, self.bas_idx)  \n",
    "            new_x = (g.x[nidx] + delta).unsqueeze(0)      \n",
    "            self._cache[key] = (y_do, new_x)\n",
    "        y_do, new_x = self._cache[key]\n",
    "        return Data(\n",
    "            x=g.x, edge_index=g.edge_index, y=g.y, num_nodes=g.num_nodes,\n",
    "            intervene_nodes=torch.tensor([nidx], dtype=torch.long, device=g.x.device),\n",
    "            new_feature_values=new_x.to(g.x.device),\n",
    "            y_do=y_do.to(g.x.device),  # supervised do-labels (synthetic ground truth)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.get(idx)\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade84982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "\n",
    "class InterventionalTrainer:\n",
    "    def __init__(self,\n",
    "                 epochs_obs=30,\n",
    "                 epochs_do=50,\n",
    "                 lr=1e-2,\n",
    "                 w_obs=0.2,\n",
    "                 w_do=1.0,\n",
    "                 weight_decay=1e-4,\n",
    "                 clip=1.0):\n",
    "        self.epochs_obs = int(epochs_obs)\n",
    "        self.epochs_do  = int(epochs_do)\n",
    "        self.lr  = float(lr)\n",
    "        self.w_obs = float(w_obs)\n",
    "        self.w_do  = float(w_do)\n",
    "        self.wd  = float(weight_decay)\n",
    "        self.clip = float(clip)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.history = []\n",
    "\n",
    "    def train(self, model, obs_loader, do_loader):\n",
    "        dev = next(model.parameters()).device\n",
    "        model = model.to(dev)\n",
    "\n",
    "        # Phase 1: observational warm-up (obs only)\n",
    "        opt = optim.AdamW(model.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        for ep in range(1, self.epochs_obs + 1):\n",
    "            model.train()\n",
    "            obs_sum, n_obs = 0.0, 0\n",
    "            for g in obs_loader:\n",
    "                g = g.to(dev)\n",
    "                pred = model(g.x, g.edge_index)\n",
    "                l_obs = self.loss(pred, g.y)\n",
    "\n",
    "                opt.zero_grad()\n",
    "                l_obs.backward()\n",
    "                if self.clip: torch.nn.utils.clip_grad_norm_(model.parameters(), self.clip)\n",
    "                opt.step()\n",
    "\n",
    "                obs_sum += float(l_obs.detach()); n_obs += 1\n",
    "\n",
    "            m_obs = obs_sum / max(n_obs, 1)\n",
    "            self.history.append({\"epoch\": ep, \"phase\": \"obs\", \"loss_obs\": m_obs, \"loss_do\": None, \"loss_total\": m_obs})\n",
    "            if ep % 10 == 0:\n",
    "                print(f\"[obs {ep:03d}] obs={m_obs:.6f}\")\n",
    "\n",
    "        # Phase 2: obs + supervised do (one combined step per batch)\n",
    "        # reset optimizer to avoid stale momentum from Phase 1\n",
    "        opt = optim.AdamW(model.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "\n",
    "        for ep in range(1, self.epochs_do + 1):\n",
    "            model.train()\n",
    "            obs_sum, do_sum, n_obs, n_do = 0.0, 0.0, 0, 0\n",
    "\n",
    "            it_obs = iter(obs_loader)\n",
    "            it_do  = iter(do_loader)\n",
    "\n",
    "            while True:\n",
    "                g_obs = next(it_obs, None)\n",
    "                g_do  = next(it_do,  None)\n",
    "                if g_obs is None and g_do is None:\n",
    "                    break\n",
    "\n",
    "                loss_terms = []\n",
    "                # observational term\n",
    "                if g_obs is not None:\n",
    "                    g_obs = g_obs.to(dev)\n",
    "                    p_obs = model(g_obs.x, g_obs.edge_index)\n",
    "                    l_obs = self.loss(p_obs, g_obs.y)\n",
    "                    loss_terms.append(self.w_obs * l_obs)\n",
    "                    obs_sum += float(l_obs.detach()); n_obs += 1\n",
    "\n",
    "                # interventional supervised term\n",
    "                if g_do is not None:\n",
    "                    g_do = g_do.to(dev)\n",
    "                    p_do = model.do_intervention(\n",
    "                        g_do.x, g_do.edge_index,\n",
    "                        intervened_nodes=g_do.intervene_nodes,\n",
    "                        new_feature_values=g_do.new_feature_values\n",
    "                    )\n",
    "                    l_do = self.loss(p_do, g_do.y_do)\n",
    "                    loss_terms.append(self.w_do * l_do)\n",
    "                    do_sum += float(l_do.detach()); n_do += 1\n",
    "\n",
    "                # combine and step once\n",
    "                total = loss_terms[0] if len(loss_terms)==1 else sum(loss_terms)\n",
    "                opt.zero_grad()\n",
    "                total.backward()\n",
    "                if self.clip: torch.nn.utils.clip_grad_norm_(model.parameters(), self.clip)\n",
    "                opt.step()\n",
    "\n",
    "            m_obs = obs_sum / max(n_obs, 1) if n_obs else 0.0\n",
    "            m_do  = do_sum  / max(n_do,  1) if n_do  else 0.0\n",
    "            total_epoch = (self.w_obs * m_obs) + (self.w_do * m_do if n_do else 0.0)\n",
    "\n",
    "            ep_abs = self.epochs_obs + ep\n",
    "            self.history.append({\"epoch\": ep_abs, \"phase\": \"do\", \"loss_obs\": m_obs, \"loss_do\": m_do, \"loss_total\": total_epoch})\n",
    "            if ep % 10 == 0:\n",
    "                print(f\"[do  {ep:03d}] total={total_epoch:.6f} (obs={m_obs:.6f}, do={m_do:.6f})\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_obs_mse(self, model, loader):\n",
    "        model.eval()\n",
    "        dev = next(model.parameters()).device\n",
    "        tot, n = 0.0, 0\n",
    "        for g in loader:\n",
    "            g = g.to(dev)\n",
    "            p = model(g.x, g.edge_index)\n",
    "            tot += self.loss(p, g.y).item(); n += 1\n",
    "        return tot / max(n,1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_do_mse(self, model, do_loader):\n",
    "        model.eval()\n",
    "        dev = next(model.parameters()).device\n",
    "        tot, n = 0.0, 0\n",
    "        for g in do_loader:\n",
    "            g = g.to(dev)\n",
    "            p = model.do_intervention(\n",
    "                g.x, g.edge_index,\n",
    "                intervened_nodes=g.intervene_nodes,\n",
    "                new_feature_values=g.new_feature_values\n",
    "            )\n",
    "            tot += self.loss(p, g.y_do).item(); n += 1\n",
    "        return tot / max(n,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944840c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt, pandas as pd, torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "obs_loader = DataLoader(ds, batch_size=1, shuffle=True)\n",
    "\n",
    "do_dataset = InterventionalDataset(\n",
    "    base_dataset=ds,\n",
    "    node_mapping=node_mapping,\n",
    "    causal_relationships=causal_relationships,\n",
    "    deltas=(+1.0, -1.0),\n",
    "    per_graph_per_node=2\n",
    ")\n",
    "do_loader = DataLoader(do_dataset, batch_size=1, shuffle=True)  \n",
    "\n",
    "g0 = ds[0]\n",
    "num_features = g0.num_node_features\n",
    "num_edges    = g0.edge_index.size(1)\n",
    "\n",
    "mcfg = cfg.get(\"model\", {})\n",
    "best_params = {\n",
    "    \"hidden_dim\": int(mcfg.get(\"hidden_dim\", 32)),\n",
    "    \"out_dim\":    int(mcfg.get(\"out_dim\", 16)),\n",
    "    \"noise_dim\":  int(mcfg.get(\"noise_dim\", 4)),\n",
    "}\n",
    "\n",
    "\n",
    "tcfg = cfg.get(\"training\", {})\n",
    "trainer_kwargs = dict(\n",
    "    epochs_obs   = int(tcfg.get(\"epochs_obs\", 40)),\n",
    "    epochs_do    = int(tcfg.get(\"epochs_do\", 20)),\n",
    "    lr           = float(tcfg.get(\"lr\", 1e-2)),\n",
    "    w_obs        = float(tcfg.get(\"w_obs\", 0.2)),\n",
    "    w_do         = float(tcfg.get(\"w_do\", 1.0)),\n",
    "    weight_decay = float(tcfg.get(\"weight_decay\", 1e-4)),\n",
    "    clip         = float(tcfg.get(\"clip\", 1.0)),\n",
    ")\n",
    "\n",
    "\n",
    "model = GNN_NCM(num_features=num_features, num_edges=num_edges, gnn_mode='per_edge', **best_params).to(device)\n",
    "trainer_do = InterventionalTrainer(**trainer_kwargs)\n",
    "trainer_do.train(model, obs_loader, do_loader)\n",
    "\n",
    "# losses\n",
    "df = pd.DataFrame(trainer_do.history)\n",
    "plt.figure(figsize=(10,5))\n",
    "if \"loss_obs\" in df: plt.plot(df[\"epoch\"], df[\"loss_obs\"], label=\"obs\")\n",
    "if \"loss_do\"  in df: plt.plot(df[\"epoch\"], df[\"loss_do\"],  label=\"do\")\n",
    "plt.plot(df[\"epoch\"], df[\"loss_total\"], label=\"total\", linewidth=2)\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.title(\"Interventional curriculum training\")\n",
    "plt.legend(); plt.grid(True, linestyle='--'); plt.tight_layout(); plt.show()\n",
    "\n",
    "print(\"obs_mse =\", trainer_do.evaluate_obs_mse(model, obs_loader))\n",
    "print(\"do_mse  =\", trainer_do.evaluate_do_mse(model, do_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d01f516",
   "metadata": {},
   "source": [
    "## 4. Ground-truth ATE Evalution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1221e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATE: do(BAS +1) on VOL\n",
    "vol_idx = node_mapping.get('VOL', 0)\n",
    "bas_idx = node_mapping.get('BAS', 0)\n",
    "\n",
    "g = ds[0].to(device)  \n",
    "with torch.no_grad():\n",
    "    dev = g.x.device\n",
    "\n",
    "    if hasattr(do_dataset, \"W\"):\n",
    "        W = do_dataset.W.to(dev, dtype=g.x.dtype)\n",
    "        bas_sem_idx = do_dataset.bas_idx if hasattr(do_dataset, \"bas_idx\") else bas_idx\n",
    "    else:\n",
    "        W, bas_sem_idx = build_sem_mats(causal_relationships, node_mapping, device=dev, dtype=g.x.dtype)\n",
    "\n",
    "    # true ATE from SEM \n",
    "    v0 = g.x.mean(dim=1)                                \n",
    "    y_true0 = sem_eval_vec(v0, W, bas_sem_idx)           \n",
    "    v1 = v0.clone(); v1[bas_idx] = v1[bas_idx] + 1.0\n",
    "    y_true1 = sem_eval_vec(v1, W, bas_sem_idx)          \n",
    "    true_ate = (y_true1 - y_true0)[vol_idx].item()\n",
    "\n",
    "    # model estimates\n",
    "    p0 = model(g.x, g.edge_index)                        \n",
    "    new_x_row = g.x[bas_idx] + 1.0                       \n",
    "    p1 = model.do_intervention(\n",
    "        g.x, g.edge_index,\n",
    "        intervened_nodes=torch.tensor([bas_idx], dtype=torch.long, device=dev),\n",
    "        new_feature_values=new_x_row.unsqueeze(0)        \n",
    "    )\n",
    "    est_ate = (p1 - p0)[vol_idx].item()\n",
    "\n",
    "df_ate = pd.DataFrame([{\"Estimated ATE (VOL)\": est_ate, \"True ATE (VOL)\": true_ate}], index=[\"do(BAS+1)\"])\n",
    "\n",
    "# Standardize df_ate to match ate_df and rename its row\n",
    "df_ate_row = df_ate.copy()\n",
    "df_ate_row.index = [\"GNN-NCM (interventional)\"]   \n",
    "df_ate_row = df_ate_row.reindex(columns=ate_df.columns) \n",
    "\n",
    "# Append as a new row (4th)\n",
    "ate_df_ext = pd.concat([ate_df, df_ate_row], axis=0)\n",
    "\n",
    "print(\"\\n--- ATE Recovery (do(BAS = +1.0) → VOL) — with Interventional GNN-NCM ---\")\n",
    "print(ate_df_ext)\n",
    "\n",
    "# Plot all four models together\n",
    "ax = ate_df_ext.plot(kind='bar', rot=0, figsize=(10,6),\n",
    "                     title=\"ATE Recovery for do(BAS = +1.0) → VOL\")\n",
    "ax.grid(axis='y', linestyle='--')\n",
    "ax.set_ylabel(\"Average Treatment Effect (ATE)\")\n",
    "ax.set_xlabel(\"Model Type\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1876cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect matrix: E[i,j] = effect on node j of do(node i, +1)\n",
    "\n",
    "dev = next(model.parameters()).device\n",
    "g = ds[0].to(dev)\n",
    "\n",
    "\n",
    "if 'W' in globals() and 'bas_sem_idx' in globals():\n",
    "    Wv, bas_idx_sem = W.to(dev, g.x.dtype), bas_sem_idx\n",
    "elif hasattr(do_dataset, \"W\"):\n",
    "    Wv, bas_idx_sem = do_dataset.W.to(dev, g.x.dtype), getattr(do_dataset, \"bas_idx\", node_mapping.get('BAS', None))\n",
    "else:\n",
    "    Wv, bas_idx_sem = build_sem_mats(causal_relationships, node_mapping, device=dev, dtype=g.x.dtype)\n",
    "\n",
    "N = g.num_nodes\n",
    "with torch.no_grad():\n",
    "    v0 = g.x.mean(dim=1)\n",
    "    y0_true = sem_eval_vec(v0, Wv, bas_idx_sem).squeeze(-1)  # [N]\n",
    "    p0 = model(g.x, g.edge_index).squeeze(-1)                # [N]\n",
    "\n",
    "E_true = torch.zeros(N, N, device=dev)\n",
    "E_est  = torch.zeros(N, N, device=dev)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(N):\n",
    "        v1 = v0.clone(); v1[i] = v1[i] + 1.0\n",
    "        y1_true = sem_eval_vec(v1, Wv, bas_idx_sem).squeeze(-1)\n",
    "        E_true[i] = (y1_true - y0_true)\n",
    "\n",
    "        new_row = g.x[i] + 1.0\n",
    "        p1 = model.do_intervention(\n",
    "            g.x, g.edge_index,\n",
    "            intervened_nodes=torch.tensor([i], dtype=torch.long, device=dev),\n",
    "            new_feature_values=new_row.unsqueeze(0)\n",
    "        ).squeeze(-1)\n",
    "        E_est[i] = (p1 - p0)\n",
    "\n",
    "# metrics\n",
    "mse_all = torch.mean((E_est - E_true)**2).item()\n",
    "corr = torch.corrcoef(torch.stack([E_true.flatten(), E_est.flatten()]))[0,1].item()\n",
    "print(f\"Effect-matrix MSE={mse_all:.6f} | Pearson r={corr:.4f}\")\n",
    "\n",
    "# heatmaps\n",
    "fig, ax = plt.subplots(1,2,figsize=(12,5))\n",
    "im0 = ax[0].imshow(E_true.detach().cpu().numpy(), aspect='auto'); ax[0].set_title(\"True effects (do +1)\")\n",
    "im1 = ax[1].imshow(E_est.detach().cpu().numpy(),  aspect='auto'); ax[1].set_title(\"Estimated effects (do +1)\")\n",
    "for a in ax: a.set_xlabel(\"target j\"); a.set_ylabel(\"intervened i\")\n",
    "fig.colorbar(im0, ax=ax[0], fraction=0.046); fig.colorbar(im1, ax=ax[1], fraction=0.046)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea51d7f5",
   "metadata": {},
   "source": [
    "## 5. Robustness Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675bd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stress degradation \n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def stress_degradation(model, loader, ops):\n",
    "    # factual MSE (same convention as your val loops)\n",
    "    mses_f = []\n",
    "    for g in loader:\n",
    "        yhat = model(g.x, g.edge_index)\n",
    "        mses_f.append(float(((yhat - g.y)**2).mean().item()))\n",
    "    mse_f = float(np.mean(mses_f))\n",
    "\n",
    "    out = []\n",
    "    for op in ops:\n",
    "        mses_s = []\n",
    "        for g in loader:\n",
    "            x = g.x.clone()\n",
    "            node = op[\"node\"]\n",
    "            new_val = op[\"value_fn\"](float(x[node, 0].item())) if \"value_fn\" in op else float(op[\"value_const\"])\n",
    "            try:\n",
    "                yhat_s = model.do_intervention(\n",
    "                    x, g.edge_index,\n",
    "                    intervened_nodes=[node],\n",
    "                    new_feature_values=torch.tensor([new_val]).float()\n",
    "                )\n",
    "            except AttributeError:\n",
    "                x[node, 0] = new_val\n",
    "                yhat_s = model(x, g.edge_index)\n",
    "            mses_s.append(float(((yhat_s - g.y)**2).mean().item()))\n",
    "        mse_s = float(np.mean(mses_s))\n",
    "        out.append({\n",
    "            \"stress_name\": op[\"name\"],\n",
    "            \"mse_factual\": mse_f,\n",
    "            \"mse_stress\": mse_s,\n",
    "            \"degradation_ratio\": float(mse_s / (mse_f + 1e-12)),\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# build ops\n",
    "BAS_IDX, MOM_IDX = node_mapping[\"BAS\"], node_mapping[\"Mom\"]\n",
    "DELTA = 0.5  # standardized units\n",
    "\n",
    "ops = [\n",
    "    {\"name\": \"do_BAS_plus_delta\", \"node\": BAS_IDX, \"value_fn\": lambda v: v + DELTA},\n",
    "    {\"name\": \"do_MOM_plus_delta\", \"node\": MOM_IDX, \"value_fn\": lambda v: v + DELTA},\n",
    "]\n",
    "\n",
    "models = {\n",
    "    \"per_edge\":                 model_per_edge.eval().cpu(),\n",
    "    \"shared\":                   model_shared.eval().cpu(),\n",
    "    \"gcn\":                      baseline_model.eval().cpu(),\n",
    "    \"GNN-NCM interventional\":   model.eval().cpu(),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdaf08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stress = {name: stress_degradation(m, val_loader, ops) for name, m in models.items()}\n",
    "\n",
    "OUT_SYN = PROJECT_ROOT / \"outputs\" / \"synthetic\"\n",
    "\n",
    "with open(os.path.join(OUT_SYN, \"stress_synthetic.json\"), \"w\") as f:\n",
    "    json.dump(stress, f, indent=2)\n",
    "print(\"saved:\", os.path.join(OUT_SYN, \"stress_synthetic.json\"))\n",
    "\n",
    "# tidy df for plotting\n",
    "rows = []\n",
    "for model_name, lst in stress.items():\n",
    "    for d in lst:\n",
    "        r = d.copy(); r[\"model\"] = model_name\n",
    "        rows.append(r)\n",
    "df = pd.DataFrame(rows)[[\"model\",\"stress_name\",\"mse_factual\",\"mse_stress\",\"degradation_ratio\"]]\n",
    "df.to_csv(os.path.join(OUT_SYN, \"stress_results_tidy.csv\"), index=False)\n",
    "\n",
    "# --- Plot 1: degradation ratio under do(BAS = BAS + Δ) ---\n",
    "plt.figure(figsize=(6,4))\n",
    "subset = df[df[\"stress_name\"]==\"do_BAS_plus_delta\"]\n",
    "plt.bar(subset[\"model\"], subset[\"degradation_ratio\"])\n",
    "plt.title(\"Synthetic — do(BAS = BAS + Δ): Degradation Ratio\")\n",
    "plt.xlabel(\"Model\"); plt.ylabel(\"MSE_stress / MSE_factual\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_SYN, \"stress_degradation_do_BAS.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 2: degradation ratio under do(Mom = Mom + Δ) ---\n",
    "plt.figure(figsize=(6,4))\n",
    "subset2 = df[df[\"stress_name\"]==\"do_MOM_plus_delta\"]\n",
    "plt.bar(subset2[\"model\"], subset2[\"degradation_ratio\"])\n",
    "plt.title(\"Synthetic — do(Mom = Mom + Δ): Degradation Ratio\")\n",
    "plt.xlabel(\"Model\"); plt.ylabel(\"MSE_stress / MSE_factual\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_SYN, \"stress_degradation_do_MOM.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 3: factual MSE by model (context for ratios) ---\n",
    "plt.figure(figsize=(6,4))\n",
    "mse_factual = df.groupby(\"model\")[\"mse_factual\"].mean().reset_index()\n",
    "plt.bar(mse_factual[\"model\"], mse_factual[\"mse_factual\"])\n",
    "plt.title(\"Synthetic — Factual MSE (Val split)\")\n",
    "plt.xlabel(\"Model\"); plt.ylabel(\"MSE (factual)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_SYN, \"factual_mse.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dc081e",
   "metadata": {},
   "source": [
    "## 6. Saving the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4d9009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pandas as pd, numpy as np\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))  \n",
    "\n",
    "OUT_SYN = PROJECT_ROOT /\"outputs\" / \"synthetic\"\n",
    "os.makedirs(OUT_SYN, exist_ok=True)\n",
    "\n",
    "ate_df_ext.to_csv(os.path.join(OUT_SYN, \"ate_table_VOL.csv\"), index=False)\n",
    "print(\"Saved synthetic ATE artifacts to\", OUT_SYN)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
