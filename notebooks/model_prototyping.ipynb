{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caea6a73",
   "metadata": {},
   "source": [
    "# Prototyping the GNN Neural Causal Model\n",
    "\n",
    "The primary objective of this notebook is to develop the core GNN_NCM class. We will take direct inspiration from CXGNN by Berham and then implement our own version using PyTorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c76fcf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\miniconda3\\envs\\ai_project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from pathlib import Path\n",
    "\n",
    "# PyTorch Geometric imports\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5fe5a5",
   "metadata": {},
   "source": [
    "### Building the GNN Neural Causal Model (GNN_NCM)\n",
    "\n",
    "Here, we define our core model class. We will include detailed comments explaining how each part corresponds to the concepts in the reference code and the broader SCM framework. The gap between SCMs and GNNs lies mainly in:\n",
    "\n",
    "1- Directionality: Causality is directed (A -> B is not the same as B -> A), but standard layers like GCNConv are inherently undirected (or symmetric).\n",
    "\n",
    "2- Modularity vs. Parameter Sharing: An SCM has a unique causal mechanism (f_ij) for each parent-child relationship, while a GNN shares its message-passing weights (ψ) across all edges for scalability.\n",
    "\n",
    "\n",
    "Therefore to improve causal fidelity we need to do build a GNN layer that is both directional and supports per-edge causal mechanisms, directly addressing the trade-off between common GNNs and SCMs. \n",
    "\n",
    "We start by implementing a simple directional message-passing layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d29c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectedGNNLayer(MessagePassing):\n",
    "    \"\"\"\n",
    "    A minimal directed message passing layer where information flows from\n",
    "    source nodes (parents) to target nodes (children).\n",
    "    \n",
    "    It uses a shared MLP for the message function (ψ) and another for the update function (φ).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        # flow='source_to_target' is the key argument that enforces directionality.\n",
    "\n",
    "        super().__init__(aggr='add', flow='source_to_target')\n",
    "\n",
    "        # ψ (psi): The message function. \n",
    "        # It computes a message based on the parent's features.\n",
    "        self.msg_mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "        \n",
    "        # φ (phi): The update function. \n",
    "        # It updates a node's representation by combining its original features (x_i) with the aggregated messages from all its parents.\n",
    "        self.update_mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim + out_dim, hidden_dim), # Takes concatenated [child, aggregated_messages]\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def message(self, x_j, x_i):\n",
    "        \"\"\"\n",
    "        Defines the message from a source node j (parent) to a target node i (child).\n",
    "        x_j is the parent's feature tensor, x_i is the child's.\n",
    "        \"\"\"\n",
    "        return self.msg_mlp(x_j)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\" The main propagation method. \"\"\"\n",
    "        # special to MessagePassing, self.propagate will call message() for each edge and aggregate() for each node.\n",
    "        aggregated_messages = self.propagate(edge_index, x=x)\n",
    "        \n",
    "        # Combine the node's original state with the messages from its parents.\n",
    "        updated_embedding = self.update_mlp(torch.cat([x, aggregated_messages], dim=-1))\n",
    "        return updated_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005a4eed",
   "metadata": {},
   "source": [
    "SCM's structural equations `x_i := f(pa(x_i), U_i)` can have a unique function `f` for each distinct causal relationship. A standard GNN uses one `msg_mlp` for all edges.\n",
    "\n",
    "We will create a GNN layer that can operate in two modes:\n",
    "1.  **`'shared'`**: Behaves like a normal GNN (efficient but less causally faithful).\n",
    "2.  **`'per_edge'`**: Instantiates a unique MLP for every single edge in the graph, (less scalable and requires more training data but causally faithful to SCMs and NCMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d31aff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeWiseGNNLayer(MessagePassing):\n",
    "    \"\"\"\n",
    "    A GNN layer that resolves the SCM modularity vs. GNN parameter sharing trade-off.\n",
    "    \n",
    "    In 'per_edge' mode, it instantiates a unique MLP for each edge, allowing it\n",
    "    to learn distinct causal mechanisms for each parent-child relationship.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_edges, mode='per_edge'):\n",
    "        super().__init__(aggr='add', flow='source_to_target')\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.num_edges = num_edges\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        if self.mode == 'per_edge':\n",
    "            # Create a list of MLPs, one for each edge. This is our f_ij.\n",
    "            # nn.ModuleList is crucial for PyTorch to recognize these as submodules.\n",
    "            self.edge_mlps = nn.ModuleList([\n",
    "                nn.Sequential(nn.Linear(in_dim, hidden_dim),nn.ReLU(),nn.Linear(hidden_dim, out_dim)) for i in range(num_edges)\n",
    "            ])\n",
    "        elif self.mode == 'shared': \n",
    "            # In shared mode, we only have one MLP for all edges.\n",
    "            self.edge_mlps = nn.Sequential(nn.Linear(in_dim, hidden_dim),nn.ReLU(),nn.Linear(hidden_dim, out_dim))\n",
    "        else:\n",
    "            raise ValueError(\"Mode must be 'shared' or 'per_edge'\")\n",
    "        \n",
    "        \n",
    "        # The update function φ remains shared across all nodes.\n",
    "        self.update_mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim + out_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, original_edge_ids=None):\n",
    "        \"\"\"\n",
    "        Executes one full message-passing step.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): The input node features.\n",
    "            edge_index (Tensor): The connectivity for this pass.\n",
    "            original_edge_ids (Tensor, optional): The original IDs of the edges in edge_index.\n",
    "                                                   If None, assumes a full graph pass.\n",
    "        \"\"\"\n",
    "        # If the original edge IDs aren't provided, we are doing a standard observational pass.\n",
    "        if original_edge_ids is None:\n",
    "            original_edge_ids = torch.arange(self.num_edges, device=x.device)\n",
    "        \n",
    "        # The propagate method will call message() and aggregate(), and its output\n",
    "        # is the aggregated messages for each node.\n",
    "        aggr_out = self.propagate(edge_index, x=x, original_edge_ids=original_edge_ids)\n",
    "        \n",
    "        # The update() method is then called to combine the aggregated messages with\n",
    "        # the original node features to produce the final node embeddings.\n",
    "        return self.update(aggr_out, x)\n",
    "\n",
    "\n",
    "    def message(self, x_j, original_edge_ids):\n",
    "        \"\"\"\n",
    "        Computes the message from parent (x_j) to child.\n",
    "        \n",
    "        Args:\n",
    "            x_j (Tensor): The feature tensor of the source nodes (parents) for each edge.\n",
    "            edge_ids (Tensor): A tensor containing the index of each edge, which we use\n",
    "                               to select the appropriate MLP.\n",
    "        \"\"\"\n",
    "        # This is a bit complex, but powerful. We can't apply all MLPs at once.\n",
    "        # We create a placeholder for the output messages.\n",
    "\n",
    "        output_messages = torch.zeros(x_j.size(0), self.out_dim, device=x_j.device)\n",
    "        if self.mode == 'per_edge':\n",
    "            for i in range(self.num_edges):\n",
    "                mask = (original_edge_ids == i)\n",
    "                if mask.any(): output_messages[mask] = self.edge_mlps[i](x_j[mask])\n",
    "            return output_messages\n",
    "        else: \n",
    "            return self.edge_mlps(x_j)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        return self.update_mlp(torch.cat([x, aggr_out], dim=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe9027",
   "metadata": {},
   "source": [
    "## The GNN-NCM Architecture: A Principled SCM Analogue\n",
    "\n",
    "First, we define our core model architecture. This consists of two main components:\n",
    "\n",
    "- EdgeWiseGNNLayer: This is our novel GNN layer that resolves the critical trade-off between SCM modularity and GNN parameter sharing. In 'per_edge' mode, it instantiates a unique MLP for each causal link in the graph, directly modeling the SCM principle that each parent -> child relationship has its own distinct mechanism f_ij.\n",
    "- GNN_NCM: This is the main model class. It orchestrates the EdgeWiseGNNLayers and explicitly implements other SCM principles, namely the inclusion of exogenous noise U and the do_intervention method for simulating interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ade3741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_NCM(nn.Module):\n",
    "    \"\"\"\n",
    "    A Graph Neural Network - Neural Causal Model (GNN-NCM).\n",
    "    \n",
    "    This model is designed to mimic a Structural Causal Model (SCM). It learns\n",
    "    from observational data and can then predict outcomes under interventions\n",
    "    (do-operations) by performing \"graph surgery\".\n",
    "    \n",
    "    This version uses a causally-faithful `EdgeWiseGNNLayer`.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, hidden_dim, out_dim, num_edges, noise_dim=4, gnn_mode=\"per_edge\"):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initializes the GNN-NCM.\n",
    "\n",
    "        Args:\n",
    "            num_features (int): The number of input features for each node.\n",
    "            hidden_dim (int): The dimensionality of the hidden MLPs inside the GNN layers.\n",
    "            out_dim (int): The output dimensionality of the GNN layers.\n",
    "            num_edges (int): ### ANNOTATION ### The total number of edges in the full graph.\n",
    "                               This is a NEW and ESSENTIAL argument for EdgeWiseGNNLayer\n",
    "                               to know how many unique MLPs to create.\n",
    "            noise_dim (int): The dimensionality of the exogenous noise vector 'U'.\n",
    "            gnn_mode (str): Either 'per_edge' for max causal fidelity or 'shared' for efficiency.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.num_edges = num_edges\n",
    "        \n",
    "        # In an SCM, x_i := f(pa(x_i), U_i). We model this by adding noise to the input.\n",
    "\n",
    "        input_dim = num_features + noise_dim\n",
    "        \n",
    "        # Instead of standard GCNConv we use our layer\n",
    "        self.conv1 = EdgeWiseGNNLayer(\n",
    "            in_dim=input_dim, \n",
    "            out_dim=hidden_dim, \n",
    "            hidden_dim=hidden_dim, # Internal MLP size\n",
    "            num_edges=num_edges, \n",
    "            mode=gnn_mode\n",
    "        )\n",
    "\n",
    "        self.conv2 = EdgeWiseGNNLayer(\n",
    "            in_dim=hidden_dim, # Input is the output of the previous layer\n",
    "            out_dim=out_dim,   # Final GNN output dimension is out_dim\n",
    "            hidden_dim=hidden_dim, \n",
    "            num_edges=num_edges, \n",
    "            mode=gnn_mode\n",
    "        )\n",
    "\n",
    "        \n",
    "        # A final linear layer to produce the output prediction (e.g., a node label probability).\n",
    "        self.out = nn.Linear(out_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Performs the standard OBSERVATIONAL forward pass.\n",
    "        \n",
    "        This simulates the system where no intervention has been done.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Node features of shape [num_nodes, num_features].\n",
    "            edge_index (Tensor): Graph connectivity in with shape [2, num_edges].\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: The output prediction for each node (e.g., logits for classification).\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Inject Exogenous Noise (U) \n",
    "        noise = torch.randn(x.size(0), self.noise_dim, device=x.device)\n",
    "        x_with_noise = torch.cat([x, noise], dim=1)\n",
    "        \n",
    "        # 2. Message Passing\n",
    "        h = F.relu(self.conv1(x_with_noise, edge_index))\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = F.relu(self.conv2(h, edge_index))\n",
    "        \n",
    "        # 3. Final Prediction\n",
    "        return self.out(h)\n",
    "\n",
    "    def do_intervention(self, x, edge_index, intervened_nodes, new_feature_values):\n",
    "        \"\"\"\n",
    "        Performs an INTERVENTIONAL forward pass, simulating a do-operation.\n",
    "        \n",
    "        This is the core of the causal inference capability. In the SCM framework, a\n",
    "        do-operation `do(X_i = v)` means we replace the mechanism that generates X_i\n",
    "        with a constant value 'v', severing the influence of its parents.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): The ORIGINAL node features before intervention.\n",
    "            edge_index (Tensor): The ORIGINAL graph structure.\n",
    "            intervened_nodes (Tensor): A 1D tensor of node indices to intervene on.\n",
    "            new_feature_values (Tensor): A tensor of new feature values to clamp onto the\n",
    "                                         intervened nodes. Shape: [num_intervened_nodes, num_features].\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The post-intervention predictions for all nodes in the graph.\n",
    "        \"\"\"\n",
    "       \n",
    "        \n",
    "        # Step 1: Clamp Node Features\n",
    "        x_intervened = x.clone()\n",
    "        x_intervened[intervened_nodes] = new_feature_values\n",
    "\n",
    "\n",
    "        # Step 2: Perform \"Graph Mutilation\"\n",
    "        # We sever all causal links (edges) pointing into the intervened node.\n",
    "        edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool, device=x.device)\n",
    "        for node_idx in intervened_nodes:\n",
    "            edge_mask &= (edge_index[1] != node_idx)\n",
    "        \n",
    "        intervened_edge_index = edge_index[:, edge_mask]\n",
    "\n",
    "\n",
    "        # Step 3: Run the forward pass on the surgically-modified graph\n",
    "        # We cannot simply call forward since EdgeWiseGNNLayer was initialized knowing about num_edges.\n",
    "\n",
    "        all_edge_ids = torch.arange(self.num_edges, device=x.device)\n",
    "        intervened_edge_ids = all_edge_ids[edge_mask]\n",
    "        \n",
    "        # Manually perform the forward pass on the modified graph structure\n",
    "        noise = torch.randn(x.size(0), self.noise_dim, device=x.device)\n",
    "        x_with_noise = torch.cat([x_intervened, noise], dim=1)\n",
    "        \n",
    "        # Pass through Layer 1\n",
    "        h1 = self.conv1(x_with_noise, intervened_edge_index, original_edge_ids=intervened_edge_ids)\n",
    "        h1 = F.relu(h1)\n",
    "        \n",
    "        # Pass through Layer 2\n",
    "        h2 = self.conv2(h1, intervened_edge_index, original_edge_ids=intervened_edge_ids)\n",
    "        h2 = F.relu(h2)\n",
    "        \n",
    "        return self.out(h2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59a9b635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN_NCM(\n",
      "  (conv1): EdgeWiseGNNLayer()\n",
      "  (conv2): EdgeWiseGNNLayer()\n",
      "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "\n",
    "# Assume you have a graph:\n",
    "num_nodes = 10\n",
    "num_features = 5\n",
    "num_edges = 25 # You must know this value beforehand\n",
    "\n",
    "# Define model dimensions\n",
    "hidden_dim = 32\n",
    "out_dim = 16\n",
    "\n",
    "# Instantiate the new, more powerful model\n",
    "causal_model = GNN_NCM(\n",
    "    num_features=num_features,\n",
    "    hidden_dim=hidden_dim,\n",
    "    out_dim=out_dim,\n",
    "    num_edges=num_edges,\n",
    "    gnn_mode='per_edge' # or 'shared'\n",
    ")\n",
    "\n",
    "print(causal_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8cea7a",
   "metadata": {},
   "source": [
    "### A Review of Causal Training Strategies\n",
    "\n",
    "Our GNN-NCM has the architectural capacity for causality, but this is useless unless it is trained in a way that respects causal principles. Since we assume we only have access to observational data, we need a clever training algorithm. Let's review the main families of approaches.\n",
    "\n",
    "* Strategy 1: Supervised Interventional Training (The Ideal Case)\n",
    "\n",
    "    As implemented, if we are lucky enough to have a dataset of (pre-state, intervention, post-state) tuples, we can directly supervise the do_intervention method.\n",
    "\n",
    "* Strategy 2: Interventional Synthesis \n",
    "\n",
    "    This is the approach pioneered by CXGNN and is the most direct fit for our SCM-like model. As detailed below, it uses the model's own interventional predictions as a way to self-supervise and derive a causally-plausible estimate of the observational outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87e2f9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_interventional_data(model, dataset, epochs=50, lr=0.001):\n",
    "    \"\"\"\n",
    "    Trains the GNN-NCM by directly supervising its `do_intervention` method.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    print(\"Starting supervised interventional training...\")\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for pre_data, intervention, post_data_true in dataset:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Use the model's causal mechanism to predict the outcome\n",
    "            post_data_pred_x = model.do_intervention(\n",
    "                x=pre_data.x,\n",
    "                edge_index=pre_data.edge_index,\n",
    "                intervened_nodes=intervention[\"node_idx\"],\n",
    "                new_feature_values=intervention[\"new_value\"]\n",
    "            )\n",
    "            \n",
    "            # The loss is how far our prediction is from the real outcome\n",
    "            loss = loss_fn(post_data_pred_x, post_data_true.x)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1:02d}, Average Interventional Loss: {total_loss / len(dataset):.6f}\")\n",
    "    print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46876b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ideal_interventional_data(num_samples=400):\n",
    "    \"\"\"\n",
    "    Creates a synthetic dataset of (pre-state, intervention, post-state) tuples.\n",
    "    We define a simple causal law: a child's feature is 0.8 * parent_feature + noise.\n",
    "    \"\"\"\n",
    "    print(\"Generating ideal interventional dataset...\")\n",
    "    # Define a fixed causal graph structure\n",
    "    edge_index = torch.tensor([[0, 0, 1], [1, 2, 3]], dtype=torch.long) # 0->1, 0->2, 1->3\n",
    "    num_nodes = 4\n",
    "    num_features = 1\n",
    "\n",
    "    dataset = []\n",
    "    for _ in range(num_samples):\n",
    "        # 1. Create a random \"pre-intervention\" state\n",
    "        pre_x = torch.randn(num_nodes, num_features)\n",
    "        \n",
    "        # 2. Define a random intervention\n",
    "        intervened_node = np.random.randint(0, num_nodes)\n",
    "        new_value = torch.randn(1, num_features)\n",
    "        \n",
    "        # 3. Calculate the true \"post-intervention\" state based on our known causal law\n",
    "        post_x = pre_x.clone()\n",
    "        post_x[intervened_node] = new_value # Clamp the intervened node's value\n",
    "        \n",
    "        # Propagate the true effects\n",
    "        for src, dst in edge_index.t().tolist():\n",
    "            if dst != intervened_node: # The effect doesn't apply if the child was intervened on\n",
    "                post_x[dst] = post_x[src] * 0.8 + torch.randn(1) * 0.1 # The ground truth causal law\n",
    "        \n",
    "        intervention_info = {\n",
    "            \"node_idx\": torch.tensor([intervened_node]),\n",
    "            \"new_value\": new_value\n",
    "        }\n",
    "        \n",
    "        dataset.append((Data(x=pre_x, edge_index=edge_index), intervention_info, Data(x=post_x)))\n",
    "        \n",
    "    return dataset, edge_index.size(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d97d9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ideal interventional dataset...\n",
      "\n",
      "--- Verifying the learned causal model ---\n",
      "Intervention: Set Node 2 to -0.13\n",
      "\n",
      "Node | True Outcome | Predicted Outcome\n",
      "---------------------------------------\n",
      "  0  |    -0.32    |      -0.16\n",
      "  1  |    -0.35    |      -0.15\n",
      "  2  |    -0.13    |      -0.16\n",
      "  3  |    -0.25    |      -0.16\n"
     ]
    }
   ],
   "source": [
    "ideal_dataset, num_edges = generate_ideal_interventional_data(num_samples=500)\n",
    "\n",
    "# Instantiate the model\n",
    "# It needs to know the total number of unique edges to build its MLPs\n",
    "model_ideal = GNN_NCM(\n",
    "    num_features=1,\n",
    "    hidden_dim=32,\n",
    "    out_dim=16,\n",
    "    num_edges=num_edges,\n",
    "    gnn_mode='per_edge'\n",
    ")\n",
    "\n",
    "# Train the model using the supervised approach\n",
    "#train_with_interventional_data(model_ideal, ideal_dataset, epochs=70)\n",
    "\n",
    "# --- Verification ---\n",
    "# Let's test if the trained model learned the causal law\n",
    "print(\"\\n--- Verifying the learned causal model ---\")\n",
    "model_ideal.eval()\n",
    "with torch.no_grad():\n",
    "    # Take a sample from our dataset\n",
    "    pre_data, intervention, post_data_true = ideal_dataset[0]\n",
    "    \n",
    "    # Get the model's prediction\n",
    "    post_data_pred = model_ideal.do_intervention(\n",
    "        pre_data.x, pre_data.edge_index, intervention['node_idx'], intervention['new_value']\n",
    "    )\n",
    "    \n",
    "    print(f\"Intervention: Set Node {intervention['node_idx'].item()} to {intervention['new_value'].item():.2f}\")\n",
    "    print(\"\\nNode | True Outcome | Predicted Outcome\")\n",
    "    print(\"---------------------------------------\")\n",
    "    for i in range(len(post_data_true.x)):\n",
    "        print(f\"  {i}  |    {post_data_true.x[i].item():.2f}    |      {post_data_pred[i].item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74af7c6f",
   "metadata": {},
   "source": [
    "## Hybrid Causal Trainer\n",
    "\n",
    "Our chosen training method is Hybrid Causal Trainer which will focus on Interventional Synthesis Principle capturing the two key actions involved:\n",
    "\n",
    "1. Intervention: We perform a series of hypothetical do-interventions on a node's parents.\n",
    "2. Synthesis: We then synthesize these individual, hypothetical outcomes (by averaging them) to reconstruct or explain the factual, observed state of the node.\n",
    "\n",
    "Here the causal loss component acts as a structural regularizer. Its entire purpose is to optimize the parameters of the causal mechanisms within our model (like the structural assumptions that SCM gives). The process is as follows:\n",
    "\n",
    "1. For a given node v, we identify its parents, pa(v).\n",
    "2. We perform a set of \"what-if\" simulations by calling model.do_intervention(parent) for each parent in pa(v).\n",
    "3. These calls produce a set of interventional predictions for v.\n",
    "4. We average these predictions to get the causally_derived_pred.\n",
    "5. The loss MSE(causally_derived_pred, stable_target) is calculated.\n",
    "6. Crucially, when loss.backward() is called, the gradient flows backwards through the averaging operation and through each of the do_intervention calls. This means the optimizer is forced to adjust the weights of the per-edge MLPs to make the causally-derived prediction more accurate.\n",
    "\n",
    "We have two possible targets for our loss function:\n",
    "\n",
    "* Target A (True Label): The ground truth label is often a \"sharp\" or \"spiky\" target (e.g., a binary 0 or 1, or a specific real value). Trying to make the average of multiple noisy outputs match this sharp target creates a chaotic and unstable gradient. The model can be pulled in many conflicting directions at once and may fail to learn.\n",
    "\n",
    "* Target B (Teacher Prediction): A pre-trained observational GNN (our \"teacher\") provides a smooth and stable target. Its prediction is a continuous value (e.g., 0.87) that represents a high-quality estimate of the conditional expectation E[Y|X]. Forcing our noisy causal estimate to match this smooth target provides a much gentler, more stable, and more informative gradient, guiding the causal mechanisms to a plausible consensus\n",
    "\n",
    "\n",
    "The teacher model is not a performance ceiling; it is a stabilizing scaffold. The primary loss_obs (which uses the true labels) is still responsible for pushing the model's overall accuracy, allowing it to surpass the teacher. Use of a teacher in calculating causal loss simply ensures that the model achieves this accuracy is by learning robust, internally consistent mechanisms.\n",
    "\n",
    "Finally our training will combine two loss functions:\n",
    "\n",
    "- Observational Loss: A standard supervised loss that compares the model's normal forward() pass prediction directly against the ground truth label (y_true). This anchors the model in reality and ensures it learns to be an accurate predictor.\n",
    "- Interventional Loss: A structural regularizer with a stable target provided by a pre-trained observational \"teacher\" model. This forces the model's internal mechanisms to be plausible and consistent.\n",
    "\n",
    "The final loss, Total Loss = Loss_obs + γ * Loss_causal, pushes the model to be accurate while ensuring its internal reasoning is causal, making it more robust and generalizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8acba2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherGNN(nn.Module):\n",
    "    \"\"\"A standard GNN to act as an observational teacher.\"\"\"\n",
    "    def __init__(self, num_features, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, out_dim)\n",
    "        self.out = nn.Linear(out_dim, 1)\n",
    "    def forward(self, x, edge_index):\n",
    "        h = F.relu(self.conv1(x, edge_index))\n",
    "        h = F.relu(self.conv2(h, edge_index))\n",
    "        return self.out(h)\n",
    "        \n",
    "class HybridCausalTrainer:\n",
    "    \"\"\"Trains a GNN-NCM using a hybrid loss for performance and causal consistency.\"\"\"\n",
    "    def __init__(self, epochs=200, lr=0.01, gamma=0.5):\n",
    "        self.epochs = epochs; self.lr = lr; self.gamma = gamma; self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def train(self, model_to_train, graph_data):\n",
    "        optimizer = optim.Adam(model_to_train.parameters(), lr=self.lr)\n",
    "        \n",
    "        # Pre-train a simple GCN \"teacher\" to provide stable observational targets\n",
    "        teacher_model = TeacherGNN(graph_data.num_features, 16, 8).to(graph_data.x.device)\n",
    "        optimizer_teacher = optim.Adam(teacher_model.parameters(), lr=0.01)\n",
    "        print(\"--- Pre-training Teacher Model ---\")\n",
    "        for _ in range(100):\n",
    "            out = teacher_model(graph_data.x, graph_data.edge_index)\n",
    "            loss = F.mse_loss(out, graph_data.y); loss.backward(); optimizer_teacher.step()\n",
    "        teacher_model.eval()\n",
    "        print(\"Teacher model trained.\")\n",
    "\n",
    "        print(\"\\n--- Starting Hybrid Causal Training ---\")\n",
    "        for epoch in range(self.epochs):\n",
    "            model_to_train.train(); optimizer.zero_grad()\n",
    "            \n",
    "            # --- 1. Observational Loss (Direct-to-Label) ---\n",
    "            obs_preds = model_to_train(graph_data.x, graph_data.edge_index)\n",
    "            loss_obs = self.loss_fn(obs_preds, graph_data.y)\n",
    "            \n",
    "            # --- 2. Causal Consistency Loss (Structural Regularizer) ---\n",
    "            loss_causal = 0\n",
    "            with torch.no_grad():\n",
    "                teacher_preds = teacher_model(graph_data.x, graph_data.edge_index).detach()\n",
    "            \n",
    "            num_causal_nodes = 0\n",
    "            for v_idx in range(graph_data.num_nodes):\n",
    "                parents = graph_data.edge_index[0][graph_data.edge_index[1] == v_idx]\n",
    "                if len(parents) > 0:\n",
    "                    num_causal_nodes += 1\n",
    "                    interventional_preds = []\n",
    "                    for parent_idx in parents:\n",
    "                        pred = model_to_train.do_intervention(\n",
    "                            graph_data.x, graph_data.edge_index,\n",
    "                            intervened_nodes=torch.tensor([parent_idx]),\n",
    "                            new_feature_values=torch.zeros(1, graph_data.num_features) \n",
    "                            # we do it with zeros so it's a neutral baseline so \n",
    "                            # it represents a no information state\n",
    "                        )\n",
    "                        interventional_preds.append(pred[v_idx])\n",
    "                    \n",
    "                    causally_derived_pred = torch.stack(interventional_preds).mean(dim=0)\n",
    "                    loss_causal += self.loss_fn(causally_derived_pred, teacher_preds[v_idx])\n",
    "\n",
    "            if num_causal_nodes > 0:\n",
    "                loss_causal /= num_causal_nodes\n",
    "            \n",
    "            # --- 3. Combined Loss ---\n",
    "            total_loss = loss_obs + self.gamma * loss_causal\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1:03d} | Total Loss: {total_loss.item():.4f} \"\n",
    "                      f\"(Obs: {loss_obs.item():.4f}, Causal: {loss_causal.item():.4f})\")\n",
    "        print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631742df",
   "metadata": {},
   "source": [
    "### Analysis on Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c763259",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.trainer'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HybridCausalTrainer\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Define the causal graph (0->1, 0->2. Node 3 is an independent root node)\u001b[39;00m\n\u001b[32m      4\u001b[39m x = torch.randn(\u001b[32m4\u001b[39m, \u001b[32m4\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src.trainer'"
     ]
    }
   ],
   "source": [
    "# Define the causal graph (0->1, 0->2. Node 3 is an independent root node)\n",
    "x = torch.randn(4, 4)\n",
    "edge_index = torch.tensor([[0, 0], [1, 2]], dtype=torch.long) # 0->1 and 0->2\n",
    "y = torch.zeros(4, 1)\n",
    "# Define ground truth such that y is a function of the MEAN of its parents' features\n",
    "y[0] = -0.5 + torch.randn(1) * 0.1 # Root node\n",
    "y[1] = x[0].mean() * 0.8 + 0.1 + torch.randn(1) * 0.1 # Child of node 0\n",
    "y[2] = torch.sin(x[0].mean()) - 0.2 + torch.randn(1) * 0.1 # Child of node 0\n",
    "y[3] = 0.5 + torch.randn(1) * 0.1 # Independent root node\n",
    "full_graph = Data(x=x, edge_index=edge_index, y=y)\n",
    "num_edges = full_graph.edge_index.size(1)\n",
    "\n",
    "# Instantiate our GNN-NCM\n",
    "causal_model = GNN_NCM(num_features=4, hidden_dim=16, out_dim=8, num_edges=num_edges)\n",
    "\n",
    "# Instantiate and run the hybrid causal trainer\n",
    "trainer = HybridCausalTrainer(epochs=200, lr=0.01, gamma=0.5)\n",
    "trainer.train(causal_model, full_graph)\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\n--- Verifying the Trained Causal Model ---\")\n",
    "causal_model.eval()\n",
    "with torch.no_grad():\n",
    "    obs_pred = causal_model(full_graph.x, full_graph.edge_index)\n",
    "    interv_pred = causal_model.do_intervention(\n",
    "        full_graph.x, full_graph.edge_index,\n",
    "        intervened_nodes=torch.tensor([0]), # Intervene on the parent\n",
    "        new_feature_values=torch.full((1, 4), 10.0) # A strong, unseen shock\n",
    "    )\n",
    "\n",
    "    print(\"\\nNode | Observational Pred | Post-Intervention Pred | Change\")\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    for i in range(full_graph.num_nodes):\n",
    "        change = interv_pred[i].item() - obs_pred[i].item()\n",
    "        print(f\"  {i}  |      {obs_pred[i].item():.4f}      |       {interv_pred[i].item():.4f}       | {change:+.4f}\")\n",
    "\n",
    "print(\"\\nVERIFICATION RESULT:\")\n",
    "print(\"Observe that the intervention on the parent (Node 0) caused a LARGE change in its children (Nodes 1 and 2).\")\n",
    "print(\"Crucially, the change in the independent root node (Node 3) is now NEAR ZERO.\")\n",
    "print(\"This confirms the model has learned the correct, directed causal structure.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
