{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caea6a73",
   "metadata": {},
   "source": [
    "# Model Prototyping: Edge-Wise Interventional GNN (GNN-NCM)\n",
    "\n",
    "**Goal.** Implement the **EdgeWiseGNNLayer** (per-edge MLP mechanisms) and training flows:\n",
    "1) observational warm-up,\n",
    "2) interventional regularization (CXGNN-inspired).\n",
    "\n",
    "We also keep **shared-weights** and **vanilla GNN** baselines for ablation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76fcf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from pathlib import Path\n",
    "\n",
    "# PyTorch Geometric imports\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5fe5a5",
   "metadata": {},
   "source": [
    "## 1. Building the GNN Neural Causal Model (GNN-NCM)\n",
    "\n",
    "Here, we define our core model class. We will include detailed comments explaining how each part corresponds to the concepts in the reference code and the broader SCM framework. The gap between SCMs and GNNs lies mainly in:\n",
    "\n",
    "1- Directionality: Causality is directed (A -> B is not the same as B -> A), but standard layers like GCNConv are inherently undirected (or symmetric).\n",
    "\n",
    "2- Modularity vs. Parameter Sharing: An SCM has a unique causal mechanism (f_ij) for each parent-child relationship, while a GNN shares its message-passing weights (ψ) across all edges for scalability.\n",
    "\n",
    "\n",
    "Therefore to improve causal fidelity we need to do build a GNN layer that is both directional and supports per-edge causal mechanisms, directly addressing the trade-off between common GNNs and SCMs. \n",
    "\n",
    "We start by implementing a simple directional message-passing layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d29c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectedGNNLayer(MessagePassing):\n",
    "    \"\"\"\n",
    "    A minimal directed message passing layer where information flows from\n",
    "    source nodes (parents) to target nodes (children).\n",
    "    \n",
    "    It uses a shared MLP for the message function (ψ) and another for the update function (φ).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        # flow='source_to_target' is the key argument that enforces directionality.\n",
    "\n",
    "        super().__init__(aggr='add', flow='source_to_target')\n",
    "\n",
    "        # ψ (psi): The message function. \n",
    "        # It computes a message based on the parent's features.\n",
    "        self.msg_mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "        \n",
    "        # φ (phi): The update function. \n",
    "        # It updates a node's representation by combining its original features (x_i) with the aggregated messages from all its parents.\n",
    "        self.update_mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim + out_dim, hidden_dim), # Takes concatenated [child, aggregated_messages]\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def message(self, x_j, x_i):\n",
    "        \"\"\"\n",
    "        Defines the message from a source node j (parent) to a target node i (child).\n",
    "        x_j is the parent's feature tensor, x_i is the child's.\n",
    "        \"\"\"\n",
    "        return self.msg_mlp(x_j)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\" The main propagation method. \"\"\"\n",
    "        # special to MessagePassing, self.propagate will call message() for each edge and aggregate() for each node.\n",
    "        aggregated_messages = self.propagate(edge_index, x=x)\n",
    "        \n",
    "        # Combine the node's original state with the messages from its parents.\n",
    "        updated_embedding = self.update_mlp(torch.cat([x, aggregated_messages], dim=-1))\n",
    "        return updated_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005a4eed",
   "metadata": {},
   "source": [
    "SCM's structural equations `x_i := f(pa(x_i), U_i)` can have a unique function `f` for each distinct causal relationship. A standard GNN uses one `msg_mlp` for all edges.\n",
    "\n",
    "We will create a GNN layer that can operate in two modes:\n",
    "1.  **`'shared'`**: Behaves like a normal GNN (efficient but less causally faithful).\n",
    "2.  **`'per_edge'`**: Instantiates a unique MLP for every single edge in the graph, (less scalable and requires more training data but causally faithful to SCMs and NCMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31aff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeWiseGNNLayer(MessagePassing):\n",
    "    \"\"\"\n",
    "    A GNN layer that resolves the SCM modularity vs. GNN parameter sharing trade-off.\n",
    "    \n",
    "    In 'per_edge' mode, it instantiates a unique MLP for each edge, allowing it\n",
    "    to learn distinct causal mechanisms for each parent-child relationship.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_edges, mode='per_edge'):\n",
    "        super().__init__(aggr='add', flow='source_to_target')\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.num_edges = num_edges\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        if self.mode == 'per_edge':\n",
    "            # Create a list of MLPs, one for each edge. This is our f_ij.\n",
    "            self.edge_mlps = nn.ModuleList([\n",
    "                nn.Sequential(nn.Linear(in_dim, hidden_dim),nn.ReLU(),nn.Linear(hidden_dim, out_dim)) for i in range(num_edges)\n",
    "            ])\n",
    "        elif self.mode == 'shared': \n",
    "            # In shared mode, we only have one MLP for all edges.\n",
    "            self.edge_mlps = nn.Sequential(nn.Linear(in_dim, hidden_dim),nn.ReLU(),nn.Linear(hidden_dim, out_dim))\n",
    "        else:\n",
    "            raise ValueError(\"Mode must be 'shared' or 'per_edge'\")\n",
    "        \n",
    "        \n",
    "        # The update function φ remains shared across all nodes.\n",
    "        self.update_mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim + out_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, original_edge_ids=None):\n",
    "        \"\"\"\n",
    "        Executes one full message-passing step.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): The input node features.\n",
    "            edge_index (Tensor): The connectivity for this pass.\n",
    "            original_edge_ids (Tensor, optional): The original IDs of the edges in edge_index.\n",
    "                                                   If None, assumes a full graph pass.\n",
    "        \"\"\"\n",
    "        # If the original edge IDs aren't provided, we are doing a standard observational pass.\n",
    "        if original_edge_ids is None:\n",
    "            original_edge_ids = torch.arange(self.num_edges, device=x.device)\n",
    "        \n",
    "        # The propagate method will call message() and aggregate(), and its output\n",
    "        # is the aggregated messages for each node.\n",
    "        aggr_out = self.propagate(edge_index, x=x, original_edge_ids=original_edge_ids)\n",
    "        \n",
    "        # The update() method is then called to combine the aggregated messages with\n",
    "        # the original node features to produce the final node embeddings.\n",
    "        return self.update(aggr_out, x)\n",
    "\n",
    "\n",
    "    def message(self, x_j, original_edge_ids):\n",
    "        \"\"\"\n",
    "        Computes the message from parent (x_j) to child.\n",
    "        \n",
    "        Args:\n",
    "            x_j (Tensor): The feature tensor of the source nodes (parents) for each edge.\n",
    "            edge_ids (Tensor): A tensor containing the index of each edge, which we use\n",
    "                               to select the appropriate MLP.\n",
    "        \"\"\"\n",
    "\n",
    "        output_messages = torch.zeros(x_j.size(0), self.out_dim, device=x_j.device)\n",
    "        if self.mode == 'per_edge':\n",
    "            for i in range(self.num_edges):\n",
    "                mask = (original_edge_ids == i)\n",
    "                if mask.any(): output_messages[mask] = self.edge_mlps[i](x_j[mask])\n",
    "            return output_messages\n",
    "        else: \n",
    "            return self.edge_mlps(x_j)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        return self.update_mlp(torch.cat([x, aggr_out], dim=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe9027",
   "metadata": {},
   "source": [
    "## 2. The GNN-NCM Architecture: A Principled SCM Analogue\n",
    "\n",
    "First, we define our core model architecture. This consists of two main components:\n",
    "\n",
    "- EdgeWiseGNNLayer: This is our novel GNN layer that resolves the critical trade-off between SCM modularity and GNN parameter sharing. In 'per_edge' mode, it instantiates a unique MLP for each causal link in the graph, directly modeling the SCM principle that each parent -> child relationship has its own distinct mechanism f_ij.\n",
    "- GNN_NCM: This is the main model class. It orchestrates the EdgeWiseGNNLayers and explicitly implements other SCM principles, namely the inclusion of exogenous noise U and the do_intervention method for simulating interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade3741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_NCM(nn.Module):\n",
    "    \"\"\"\n",
    "    A Graph Neural Network - Neural Causal Model (GNN-NCM).\n",
    "    \n",
    "    This model is designed to mimic a Structural Causal Model (SCM). It learns\n",
    "    from observational data and can then predict outcomes under interventions\n",
    "    (do-operations) by performing \"graph surgery\".\n",
    "    \n",
    "    This version uses a causally-faithful `EdgeWiseGNNLayer`.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, hidden_dim, out_dim, num_edges, noise_dim=4, gnn_mode=\"per_edge\"):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initializes the GNN-NCM.\n",
    "\n",
    "        Args:\n",
    "            num_features (int): The number of input features for each node.\n",
    "            hidden_dim (int): The dimensionality of the hidden MLPs inside the GNN layers.\n",
    "            out_dim (int): The output dimensionality of the GNN layers.\n",
    "            num_edges (int): ### ANNOTATION ### The total number of edges in the full graph.\n",
    "                               This is a NEW and ESSENTIAL argument for EdgeWiseGNNLayer\n",
    "                               to know how many unique MLPs to create.\n",
    "            noise_dim (int): The dimensionality of the exogenous noise vector 'U'.\n",
    "            gnn_mode (str): Either 'per_edge' for max causal fidelity or 'shared' for efficiency.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.num_edges = num_edges\n",
    "        \n",
    "        # In an SCM, x_i := f(pa(x_i), U_i). We model this by adding noise to the input.\n",
    "\n",
    "        input_dim = num_features + noise_dim\n",
    "        \n",
    "        # Instead of standard GCNConv we use our layer\n",
    "        self.conv1 = EdgeWiseGNNLayer(\n",
    "            in_dim=input_dim, \n",
    "            out_dim=hidden_dim, \n",
    "            hidden_dim=hidden_dim, \n",
    "            num_edges=num_edges, \n",
    "            mode=gnn_mode\n",
    "        )\n",
    "\n",
    "        self.conv2 = EdgeWiseGNNLayer(\n",
    "            in_dim=hidden_dim, \n",
    "            out_dim=out_dim,   \n",
    "            hidden_dim=hidden_dim, \n",
    "            num_edges=num_edges, \n",
    "            mode=gnn_mode\n",
    "        )\n",
    "\n",
    "        \n",
    "        # A final linear layer to produce the output prediction\n",
    "        self.out = nn.Linear(out_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Performs the standard OBSERVATIONAL forward pass.\n",
    "        \n",
    "        This simulates the system where no intervention has been done.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Node features of shape [num_nodes, num_features].\n",
    "            edge_index (Tensor): Graph connectivity in with shape [2, num_edges].\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: The output prediction for each node (e.g., logits for classification).\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Inject Exogenous Noise (U) \n",
    "        noise = torch.randn(x.size(0), self.noise_dim, device=x.device)\n",
    "        x_with_noise = torch.cat([x, noise], dim=1)\n",
    "        \n",
    "        # 2. Message Passing\n",
    "        h = F.relu(self.conv1(x_with_noise, edge_index))\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = F.relu(self.conv2(h, edge_index))\n",
    "        \n",
    "        # 3. Final Prediction\n",
    "        return self.out(h)\n",
    "\n",
    "    def do_intervention(self, x, edge_index, intervened_nodes, new_feature_values):\n",
    "        \"\"\"\n",
    "        Performs an INTERVENTIONAL forward pass, simulating a do-operation.\n",
    "        \n",
    "        This is the core of the causal inference capability. In the SCM framework, a\n",
    "        do-operation `do(X_i = v)` means we replace the mechanism that generates X_i\n",
    "        with a constant value 'v', severing the influence of its parents.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): The ORIGINAL node features before intervention.\n",
    "            edge_index (Tensor): The ORIGINAL graph structure.\n",
    "            intervened_nodes (Tensor): A 1D tensor of node indices to intervene on.\n",
    "            new_feature_values (Tensor): A tensor of new feature values to clamp onto the\n",
    "                                         intervened nodes. Shape: [num_intervened_nodes, num_features].\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The post-intervention predictions for all nodes in the graph.\n",
    "        \"\"\"\n",
    "       \n",
    "        \n",
    "        # Step 1: Clamp Node Features\n",
    "        x_intervened = x.clone()\n",
    "        x_intervened[intervened_nodes] = new_feature_values\n",
    "\n",
    "\n",
    "        # Step 2: Perform \"Graph Mutilation\"\n",
    "        # We sever all causal links (edges) pointing into the intervened node.\n",
    "        edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool, device=x.device)\n",
    "        for node_idx in intervened_nodes:\n",
    "            edge_mask &= (edge_index[1] != node_idx)\n",
    "        \n",
    "        intervened_edge_index = edge_index[:, edge_mask]\n",
    "\n",
    "\n",
    "        # Step 3: Run the forward pass on the surgically-modified graph\n",
    "        # We cannot simply call forward since EdgeWiseGNNLayer was initialized knowing about num_edges.\n",
    "\n",
    "        all_edge_ids = torch.arange(self.num_edges, device=x.device)\n",
    "        intervened_edge_ids = all_edge_ids[edge_mask]\n",
    "        \n",
    "        # Manually perform the forward pass on the modified graph structure\n",
    "        noise = torch.randn(x.size(0), self.noise_dim, device=x.device)\n",
    "        x_with_noise = torch.cat([x_intervened, noise], dim=1)\n",
    "        \n",
    "        # Pass through Layer 1\n",
    "        h1 = self.conv1(x_with_noise, intervened_edge_index, original_edge_ids=intervened_edge_ids)\n",
    "        h1 = F.relu(h1)\n",
    "        \n",
    "        # Pass through Layer 2\n",
    "        h2 = self.conv2(h1, intervened_edge_index, original_edge_ids=intervened_edge_ids)\n",
    "        h2 = F.relu(h2)\n",
    "        \n",
    "        return self.out(h2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a9b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "# Assume you have a graph:\n",
    "num_nodes = 10\n",
    "num_features = 5\n",
    "num_edges = 25 \n",
    "\n",
    "# Define model dimensions\n",
    "hidden_dim = 32\n",
    "out_dim = 16\n",
    "\n",
    "# Instantiate the model\n",
    "causal_model = GNN_NCM(\n",
    "    num_features=num_features,\n",
    "    hidden_dim=hidden_dim,\n",
    "    out_dim=out_dim,\n",
    "    num_edges=num_edges,\n",
    "    gnn_mode='per_edge' # or 'shared'\n",
    ")\n",
    "\n",
    "print(causal_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8cea7a",
   "metadata": {},
   "source": [
    "### 3. A Review of Causal Training Strategies\n",
    "\n",
    "Our GNN-NCM has the architectural capacity for causality, but this is useless unless it is trained in a way that respects causal principles. Since we assume we only have access to observational data, we need a clever training algorithm. Let's review the main families of approaches.\n",
    "\n",
    "* Strategy 1: Supervised Interventional Training (The Ideal Case)\n",
    "\n",
    "    As implemented, if we are lucky enough to have a dataset of (pre-state, intervention, post-state) tuples, we can directly supervise the do_intervention method. We analyize this more in the synthetic dataset notebook\n",
    "\n",
    "\n",
    "* Strategy 2: Interventional Synthesis \n",
    "\n",
    "    This is the approach pioneered by CXGNN and is the most direct fit for our SCM-like model. As detailed below, it uses the model's own interventional predictions as a way to self-supervise and derive a causally-plausible estimate of the observational outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4999b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "\n",
    "class CausalTwoPartTrainer:\n",
    "    def __init__(self,\n",
    "                 epochs_obs=30,\n",
    "                 epochs_do=150,\n",
    "                 lr=5e-3,\n",
    "                 w_obs=0.2,\n",
    "                 w_do=1.0,\n",
    "                 weight_decay=1e-4,\n",
    "                 clip=1.0,\n",
    "                 neutral='zeros',     # 'zeros' or 'self+delta'\n",
    "                 delta=0.0):          # scalar or 1D tensor of size F (used when neutral == 'self+delta')\n",
    "        self.epochs_obs = int(epochs_obs)\n",
    "        self.epochs_do  = int(epochs_do)\n",
    "        self.lr  = float(lr)\n",
    "        self.w_obs = float(w_obs)\n",
    "        self.w_do  = float(w_do)\n",
    "        self.wd  = float(weight_decay)\n",
    "        self.clip = float(clip)\n",
    "\n",
    "        self.neutral = str(neutral)\n",
    "        self.delta = delta\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.history = []\n",
    "\n",
    "    \n",
    "    def train(self, model, loader, val_loader=None):\n",
    "        dev = next(model.parameters()).device\n",
    "        model = model.to(dev)\n",
    "\n",
    "        # Phase 1: observational warm-up (obs only) \n",
    "        opt = optim.AdamW(model.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        for ep in range(1, self.epochs_obs + 1):\n",
    "            model.train()\n",
    "            obs_sum, n_obs = 0.0, 0\n",
    "            for g in loader:\n",
    "                g = g.to(dev)\n",
    "                pred = model(g.x, g.edge_index)\n",
    "                l_obs = self.loss(pred, g.y)\n",
    "\n",
    "                opt.zero_grad()\n",
    "                l_obs.backward()\n",
    "                if self.clip: torch.nn.utils.clip_grad_norm_(model.parameters(), self.clip)\n",
    "                opt.step()\n",
    "\n",
    "                obs_sum += float(l_obs.detach()); n_obs += 1\n",
    "\n",
    "            m_obs = obs_sum / n_obs\n",
    "            m_val= self.evaluate_obs_mse(model, val_loader)\n",
    "\n",
    "            self.history.append({\n",
    "                \"epoch\": ep, \"phase\": \"obs\",\n",
    "                \"loss_obs\": m_obs, \"loss_do\": None, \"loss_total\": m_obs,\n",
    "                \"val_obs\": m_val\n",
    "            })\n",
    "            if ep % 10 == 0:\n",
    "                msg = f\"[obs {ep:03d}] obs={m_obs:.6f}\"\n",
    "                if m_val is not None: msg += f\" | val_obs={m_val:.6f}\"\n",
    "                print(msg)\n",
    "\n",
    "\n",
    "        # Phase 2: obs + causal (one combined step per batch)\n",
    "        # reset optimizer to avoid stale momentum from Phase 1\n",
    "        opt = optim.AdamW(model.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "\n",
    "        \n",
    "        for ep in range(1, self.epochs_do + 1):\n",
    "            model.train()\n",
    "            obs_sum, do_sum, n_obs, n_do = 0.0, 0.0, 0, 0\n",
    "\n",
    "            for g in loader:\n",
    "                g = g.to(dev)\n",
    "                x, edge_index, y = g.x, g.edge_index, g.y\n",
    "\n",
    "                # observational term\n",
    "                p_obs = model(x, edge_index)\n",
    "                l_obs = self.loss(p_obs, y)\n",
    "                obs_sum += float(l_obs.detach()); n_obs += 1\n",
    "\n",
    "                # causal term: do(parent) one at a time, aggregate to each child\n",
    "                l_cau = self._causal_loss_do_parent_average(model, g, p_obs)\n",
    "                do_sum += float(l_cau.detach()); n_do += 1\n",
    "\n",
    "                # combine and step once\n",
    "                total = (self.w_obs * l_obs) + (self.w_do * l_cau)\n",
    "                opt.zero_grad()\n",
    "                total.backward()\n",
    "                if self.clip: torch.nn.utils.clip_grad_norm_(model.parameters(), self.clip)\n",
    "                opt.step()\n",
    "\n",
    "            m_obs = obs_sum / max(n_obs, 1) if n_obs else 0.0\n",
    "            m_do  = do_sum  / max(n_do,  1) if n_do  else 0.0\n",
    "            total_epoch = (self.w_obs * m_obs) + (self.w_do * m_do if n_do else 0.0)\n",
    "\n",
    "            m_val= self.evaluate_obs_mse(model, val_loader)\n",
    "\n",
    "            ep_abs = self.epochs_obs + ep\n",
    "            self.history.append({\n",
    "                \"epoch\": ep_abs, \"phase\": \"do\",\n",
    "                \"loss_obs\": m_obs, \"loss_do\": m_do, \"loss_total\": total_epoch,\n",
    "                \"val_obs\": m_val\n",
    "            })\n",
    "            if ep % 10 == 0:\n",
    "                msg = f\"[do  {ep:03d}] total={total_epoch:.6f} (obs={m_obs:.6f}, do={m_do:.6f})\"\n",
    "                if m_val is not None: msg += f\" | val_obs={m_val:.6f}\"\n",
    "                print(msg)\n",
    "\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _causal_loss_do_parent_average(self, model, g, p_obs):\n",
    "        \"\"\"\n",
    "        Implements causal loss:\n",
    "          - For each UNIQUE parent node p (from edges src->dst), compute prediction under do(p)\n",
    "            where features of p are set to a neutral row ('zeros' or 'self+delta').\n",
    "          - For each child v, average the do(p)[v] over all parents p of v to form a target.\n",
    "          - Compare that target to TRUE y[v] (MSE), average over nodes that have parents.\n",
    "\n",
    "        \"\"\"\n",
    "        dev = p_obs.device\n",
    "        x, edge_index, y = g.x, g.edge_index, g.y\n",
    "        N, F = x.size(0), x.size(1)\n",
    "\n",
    "\n",
    "        src, dst = edge_index[0], edge_index[1]\n",
    "        if src.numel() == 0:\n",
    "            # no edges -> no parents -> causal term 0 \n",
    "            return 0.0 * p_obs.sum()\n",
    "\n",
    "        unique_parents = torch.unique(src)\n",
    "\n",
    "        # prepare delta row if needed\n",
    "        if isinstance(self.delta, torch.Tensor):\n",
    "            delta_row = self.delta.to(device=dev, dtype=x.dtype)\n",
    "        else:\n",
    "            delta_row = torch.full((F,), float(self.delta), device=dev, dtype=x.dtype)\n",
    "\n",
    "        # compute do(parent) predictions once per parent\n",
    "        p1_map = {}\n",
    "        for p in unique_parents.tolist():\n",
    "            p = int(p)\n",
    "            if self.neutral == 'zeros':\n",
    "                new_row = torch.zeros(F, device=dev, dtype=x.dtype)\n",
    "            else:  # 'self+delta'\n",
    "                new_row = x[p] + delta_row\n",
    "\n",
    "            if hasattr(model, \"do_intervention\") and callable(getattr(model, \"do_intervention\")):\n",
    "                p1 = model.do_intervention(\n",
    "                    x, edge_index,\n",
    "                    intervened_nodes=torch.tensor([p], dtype=torch.long, device=dev),\n",
    "                    new_feature_values=new_row.unsqueeze(0)\n",
    "                ) \n",
    "            else:\n",
    "                # fallback: override node p's features and run a forward pass\n",
    "                x_do = x.clone()\n",
    "                x_do[p] = new_row\n",
    "                p1 = model(x_do, edge_index) \n",
    "            p1_map[p] = p1\n",
    "\n",
    "        # aggregate targets per child and compare to TRUE y[v]\n",
    "        loss_causal = 0.0\n",
    "        count = 0\n",
    "        for v in range(N):\n",
    "            parents_v = src[dst == v]\n",
    "            if parents_v.numel() == 0:\n",
    "                continue\n",
    "            vals = []\n",
    "            for p in parents_v.tolist():\n",
    "                if p in p1_map:\n",
    "                    vals.append(p1_map[p][v])  # prediction at node v under do(p)\n",
    "            if not vals:\n",
    "                continue\n",
    "            target_v = torch.stack(vals, dim=0).mean(dim=0)  # averaged target for node v\n",
    "            loss_causal = loss_causal + self.loss(target_v, y[v])\n",
    "            count += 1\n",
    "\n",
    "        return (loss_causal / count) if count > 0 else (0.0 * p_obs.sum())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_obs_mse(self, model, loader):\n",
    "        model.eval()\n",
    "        dev = next(model.parameters()).device\n",
    "        tot, n = 0.0, 0\n",
    "        for g in loader:\n",
    "            g = g.to(dev)\n",
    "            p = model(g.x, g.edge_index)  \n",
    "            y = g.y                        \n",
    "            if p.dim()==2 and p.size(-1)==1: p = p.squeeze(-1)\n",
    "            if y.dim()==2 and y.size(-1)==1: y = y.squeeze(-1)\n",
    "\n",
    "\n",
    "            tot += self.loss(p, y).item(); n += 1\n",
    "        return tot / max(n, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288ad26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))  \n",
    "\n",
    "PROCCESSED_DATA_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "PROCCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from src.dataloader import CausalFactorDataset\n",
    "\n",
    "full_dataset = CausalFactorDataset(root_dir=PROCCESSED_DATA_DIR, target_node=\"VOL\", feature_col=None, drop_self_for_target=True)\n",
    "\n",
    "# --- use it with a DataLoader and plot ---\n",
    "import pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "split = int(0.8*len(full_dataset))\n",
    "train_loader = DataLoader(Subset(full_dataset, range(split)), batch_size=1, shuffle=True)\n",
    "val_loader   = DataLoader(Subset(full_dataset, range(split, len(full_dataset))), batch_size=1, shuffle=False)\n",
    "\n",
    "# dims\n",
    "g0 = next(iter(train_loader))\n",
    "num_features = g0.num_node_features\n",
    "num_edges    = g0.edge_index.size(1)\n",
    "\n",
    "# model\n",
    "model = GNN_NCM(num_features=num_features, num_edges=num_edges, gnn_mode='per_edge',\n",
    "                hidden_dim=32, out_dim=16).to(g0.x.device)\n",
    "\n",
    "# trainer (two-phase, fixed gamma)\n",
    "trainer = CausalTwoPartTrainer(\n",
    "    epochs_obs=40, epochs_do=20, w_obs=0.2, w_do=1.0,\n",
    "    neutral='zeros',           # or 'self+delta'\n",
    "    delta=0.1                  # scalar or 1D tensor (used only if neutral='self+delta')\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train(model, train_loader, val_loader=val_loader)  \n",
    "\n",
    "# plot\n",
    "df = pd.DataFrame(trainer.history)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "if \"val_obs\" in df and df[\"val_obs\"].notna().any():\n",
    "    plt.plot(df[\"epoch\"], df[\"val_obs\"], label=\"val_obs\", linewidth=3)\n",
    "if \"loss_obs\" in df: plt.plot(df[\"epoch\"], df[\"loss_obs\"], label=\"train_obs\", alpha=0.6)\n",
    "if \"loss_total\" in df: plt.plot(df[\"epoch\"], df[\"loss_total\"], label=\"train_total\", alpha=0.6)\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.title(\"Validation loss over training\")\n",
    "plt.legend(); plt.grid(True, linestyle='--'); plt.tight_layout(); plt.show()\n",
    "\n",
    "print(\"final val_obs =\", trainer.evaluate_obs_mse(model, val_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7b3924",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aff412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from itertools import product\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "def make_loaders(dataset, split=0.8, batch_size=1):\n",
    "    n = len(dataset)\n",
    "    s = int(split * n)\n",
    "    train_idx = list(range(s))\n",
    "    val_idx   = list(range(s, n))\n",
    "    train_loader = DataLoader(Subset(dataset, train_idx), batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(Subset(dataset, val_idx),   batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def sample_config(rng, space):\n",
    "    \"\"\"Sample one config from a mixed search space with discrete choices and ranges.\"\"\"\n",
    "    cfg = {}\n",
    "    for k, spec in space.items():\n",
    "        if isinstance(spec, list):\n",
    "            cfg[k] = rng.choice(spec)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported search spec for {k}: {spec}\")\n",
    "    return cfg\n",
    "\n",
    "def tune_hyperparameters_random(dataset, n_trials=40, seed=42, val_node_key=None):\n",
    "    # Set seeds for reproducibility\n",
    "    rng = np.random.default_rng(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    train_loader, val_loader = make_loaders(dataset, split=0.8, batch_size=1)\n",
    "    g0 = dataset[0]\n",
    "    num_features = g0.num_node_features\n",
    "    num_edges    = g0.edge_index.size(1)\n",
    "    device       = g0.x.device\n",
    "\n",
    "\n",
    "    # define a search space\n",
    "    search_space = {\n",
    "        # model\n",
    "        \"hidden_dim\": [16, 32, 64],\n",
    "        \"out_dim\":    [8, 16, 32],\n",
    "\n",
    "        # optimizer\n",
    "        \"lr\": [1e-4, 5e-3, 1e-3, 1e-2],            \n",
    "        \"weight_decay\": [1e-4, 1e-3],  \n",
    "\n",
    "        # trainer knobs\n",
    "        \"epochs_obs\": [20, 40, 60],\n",
    "        \"epochs_do\":  [10, 20, 30],\n",
    "        \"w_obs\":      [0.1, 0.2, 0.5],\n",
    "        \"w_do\":       [0.5, 1.0, 2.0],\n",
    "        \"clip\":       [0.5, 1.0, 2.0],\n",
    "        \"neutral\":    [\"zeros\", \"self+delta\"],\n",
    "        \"delta\":      [0.0, 0.1, 1.0],   # only used if neutral == 'self+delta'\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    best = {\"val\": float(\"inf\"), \"cfg\": None, \"history\": None}\n",
    "\n",
    "    print(f\"Starting random search with {n_trials} trials...\")\n",
    "    for t in tqdm(range(n_trials), desc=\"Random Search\"):\n",
    "        # Sample a random configuration in each trial\n",
    "        cfg = sample_config(rng, search_space)\n",
    "\n",
    "        # build model\n",
    "        model = GNN_NCM(num_features=num_features, num_edges=num_edges,\n",
    "                        gnn_mode='per_edge',\n",
    "                        hidden_dim=int(cfg[\"hidden_dim\"]),\n",
    "                        out_dim=int(cfg[\"out_dim\"])).to(device)\n",
    "\n",
    "        # build trainer\n",
    "        trainer = CausalTwoPartTrainer(\n",
    "            epochs_obs=int(cfg[\"epochs_obs\"]),\n",
    "            epochs_do=int(cfg[\"epochs_do\"]),\n",
    "            lr=float(cfg[\"lr\"]),\n",
    "            w_obs=float(cfg[\"w_obs\"]),\n",
    "            w_do=float(cfg[\"w_do\"]),\n",
    "            weight_decay=float(cfg[\"weight_decay\"]),\n",
    "            clip=float(cfg[\"clip\"]),\n",
    "            neutral=cfg[\"neutral\"],\n",
    "            delta=float(cfg[\"delta\"])\n",
    "        )\n",
    "\n",
    "        # train with validation tracking\n",
    "    \n",
    "        trainer.train(model, train_loader, val_loader=val_loader)\n",
    "\n",
    "        # choose best validation obs over epochs\n",
    "        df = pd.DataFrame(trainer.history)\n",
    "        if \"val_obs\" in df and df[\"val_obs\"].notna().any():\n",
    "            val_best = float(df[\"val_obs\"].min())\n",
    "        else:\n",
    "            # fallback: compute once\n",
    "            val_best = float(trainer.evaluate_obs_mse(model, val_loader))\n",
    "\n",
    "        results.append({\"trial\": t, \"val_best\": val_best, \"cfg\": cfg})\n",
    "\n",
    "        if val_best < best[\"val\"]:\n",
    "            best = {\"val\": val_best, \"cfg\": cfg, \"history\": trainer.history}\n",
    "            print(f\"New best @ trial {t}: val={val_best:.6f} cfg={cfg}\")\n",
    "\n",
    "    # pretty-print best\n",
    "    print(\"\\n--- Hyperparameter Tuning Complete ---\")\n",
    "    print(f\"Best validation loss: {best['val']:.6f}\")\n",
    "    print(\"Best hyperparameters:\")\n",
    "\n",
    "    best_cfg = {\n",
    "        \"experiment_name\": \"GNN_NCM_TwoPart_VOL\",\n",
    "        \"seed\": seed,\n",
    "        \"device\": \"cuda\",\n",
    "        \"data\": {\n",
    "            \"target_node\": \"VOL\",\n",
    "            \"batch_size\": 1,\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"name\": \"GNN_NCM\",\n",
    "            \"gnn_mode\": \"per_edge\",\n",
    "            \"hidden_dim\": int(best[\"cfg\"][\"hidden_dim\"]),\n",
    "            \"out_dim\": int(best[\"cfg\"][\"out_dim\"]),\n",
    "            \"noise_dim\": 4,\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"type\": \"CausalTwoPartTrainer\",\n",
    "            \"epochs_obs\": int(best[\"cfg\"][\"epochs_obs\"]),\n",
    "            \"epochs_do\":  int(best[\"cfg\"][\"epochs_do\"]),\n",
    "            \"lr\": float(best[\"cfg\"][\"lr\"]),\n",
    "            \"weight_decay\": float(best[\"cfg\"][\"weight_decay\"]),\n",
    "            \"clip\": float(best[\"cfg\"][\"clip\"]),\n",
    "            \"w_obs\": float(best[\"cfg\"][\"w_obs\"]),\n",
    "            \"w_do\":  float(best[\"cfg\"][\"w_do\"]),\n",
    "            \"neutral\": str(best[\"cfg\"][\"neutral\"]),\n",
    "            \"delta\": float(best[\"cfg\"][\"delta\"]),\n",
    "        },\n",
    "        \"output\": {\"output_dir\": \"outputs\"},\n",
    "    }\n",
    "\n",
    "\n",
    "    return best_cfg, results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffd0850",
   "metadata": {},
   "source": [
    "### Saving the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04aa80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the tuning process\n",
    "best, results = tune_hyperparameters_random(full_dataset, n_trials=40, seed=42)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bf5f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final config file\n",
    "# Ensure the configs directory exists\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))  \n",
    "\n",
    "CONFIG = PROJECT_ROOT / \"configs\" /\"best_config.yaml\"\n",
    "\n",
    "with open(CONFIG, 'w') as f:\n",
    "    yaml.dump(best, f, indent=2, sort_keys=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
