{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98158bb4",
   "metadata": {},
   "source": [
    "# Preprocessing & Graph Construction\n",
    "\n",
    "**Goal.** Build the day-by-day graph dataset used throughout the thesis:\n",
    "- one **graph per date**,\n",
    "- **fixed nodes**: `MOM, HML, OI, PC, BAS, LIQ, VOL`,\n",
    "- **directed edges** encode the assumed causal diagram,\n",
    "- features are **scalar node values for that date**,\n",
    "- target is next-day (or same-day, if you chose) `VOL`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacae08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from pandas_datareader import data as pdr\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# config\n",
    "MARKET_PROXY_TICKER = \"SPY\"\n",
    "ALL_NODES = [\"Mom\", \"HML\", \"OI\", \"PC\", \"BAS\", \"LIQ\", \"VOL\"]\n",
    "START_DATE = \"2023-01-01\"\n",
    "END_DATE = \"2023-12-31\"\n",
    "VOL_WINDOW = 21\n",
    "ROLL_SPREAD_WINDOW = 21\n",
    "\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Project Root detected at: {PROJECT_ROOT}\")\n",
    "print(f\"Libraries loaded. Output will be saved to '{OUTPUT_DIR}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0603fbd5",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering (Definitions)\n",
    "\n",
    "We follow standard operational definitions:\n",
    "\n",
    "- **MOM (Momentum).** Winners minus losers over prior 12 months, skip last month.\n",
    "- **HML (Value).** High minus low book-to-market proxy.\n",
    "- **OI (Order Imbalance).** Net buyer-initiated minus seller-initiated flow.\n",
    "- **PC (Price Change).** Same-day percentage return.\n",
    "- **BAS (Bid–Ask Spread).** Quoted or effective spread proxy.\n",
    "- **LIQ (Liquidity).** Inverse illiquidity proxy (choose your preferred measure).\n",
    "- **VOL (Realized Volatility).** Rolling window realized vol or daily range-based proxy.\n",
    "\n",
    "To get data for these abstract nodes, we will use a market proxy—the SPDR S&P 500 ETF (SPY)—to calculate all price-derived and microstructure variables. The factor data comes directly from the Kenneth French Data Library. All required metrics can be calculated from simple daily data. Then we calculate the values using the helper functions we created and scale the variables with too big or too small values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf129544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions \n",
    "def compute_returns(df, price_col):\n",
    "    return df[price_col].pct_change()\n",
    "\n",
    "def compute_rolling_vol(returns, window=21):\n",
    "    return returns.rolling(window=window, min_periods=window).std()\n",
    "\n",
    "def compute_amihud(return_series, dollar_volume_series):\n",
    "    return (return_series.abs() / (dollar_volume_series + 1e-9)).fillna(0.0)\n",
    "\n",
    "def compute_signed_volume(return_series, dollar_volume_series):\n",
    "    return np.sign(return_series).fillna(0) * dollar_volume_series\n",
    "\n",
    "def compute_roll_spread(price_series, window=21):\n",
    "    dp = price_series.diff()\n",
    "    dp_lag = dp.shift(1)\n",
    "    cov = dp.rolling(window=window).cov(dp_lag)\n",
    "    spread = 2.0 * np.sqrt((-cov).clip(lower=0.0))\n",
    "    return spread.fillna(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc29f214",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading Fama-French 3 Factors and Momentum Factor (Daily)...\")\n",
    "ff_3_factor = pdr.DataReader('F-F_Research_Data_Factors_daily', 'famafrench', start=START_DATE, end=END_DATE)[0]\n",
    "ff_mom_factor = pdr.DataReader('F-F_Momentum_Factor_daily', 'famafrench', start=START_DATE, end=END_DATE)[0]\n",
    "\n",
    "\n",
    "# Combine and clean the factor data\n",
    "ff_factors = ff_3_factor.join(ff_mom_factor, how='inner')\n",
    "ff_factors /= 100.0\n",
    "\n",
    "ff_factors = ff_factors[['HML', 'Mom']].copy()\n",
    "print(\"Fama-French data downloaded successfully.\")\n",
    "\n",
    "print(f\"\\nDownloading market proxy data for {MARKET_PROXY_TICKER}...\")\n",
    "market_data_raw = yf.download(MARKET_PROXY_TICKER, start=START_DATE, end=END_DATE)\n",
    "print(\"Market proxy data downloaded successfully.\")\n",
    "\n",
    "if isinstance(market_data_raw.columns, pd.MultiIndex):\n",
    "    market_data = market_data_raw.xs(MARKET_PROXY_TICKER, level='Ticker', axis=1).copy()\n",
    "else:\n",
    "    market_data = market_data_raw.copy()\n",
    "\n",
    "market_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339caddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_col = 'Price' if 'Price' in market_data.columns else 'Close'\n",
    "print(f\"\\nUsing '{price_col}' as the primary price column for calculations.\")\n",
    "\n",
    "market_data['pc'] = compute_returns(market_data, price_col=price_col)\n",
    "market_data['vol'] = compute_rolling_vol(market_data['pc'], window=VOL_WINDOW)\n",
    "market_data['dollarvolume'] = market_data[price_col] * market_data['Volume']\n",
    "market_data['liq'] = compute_amihud(market_data['pc'], market_data['dollarvolume'])\n",
    "market_data['bas'] = compute_roll_spread(market_data[price_col], window=ROLL_SPREAD_WINDOW)\n",
    "market_data['oi'] = compute_signed_volume(market_data['pc'], market_data['dollarvolume'])\n",
    "\n",
    "# Rename columns to match our final node names for clarity\n",
    "market_data.rename(columns={\n",
    "    'pc': 'PC', 'vol': 'VOL', 'liq': 'LIQ', 'bas': 'BAS', 'oi': 'OI'\n",
    "}, inplace=True)\n",
    "\n",
    "# Join the factor data with the calculated microstructure data\n",
    "final_df_wide = ff_factors.join(market_data, how='inner').dropna()\n",
    "final_df_wide = final_df_wide[ALL_NODES]\n",
    "print(\"\\nAll node values calculated and aligned.\")\n",
    "final_df_wide.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e6d3d",
   "metadata": {},
   "source": [
    "Our preprocessing employs a targeted scaling strategy, transforming only variables with extreme ranges to ensure the numerical stability of our models while preserving interpretability. Specifically, we apply a signed logarithmic transformation to compress the high-magnitude `OI` variable and rescale the low-magnitude `LIQ` and `VOL` variables by a constant factor. All other variables remain on their original scale, ensuring their estimated causal effects are directly interpretable in their real-world units.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d415a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = final_df_wide.copy()\n",
    "\n",
    "df_t['OI'] = np.sign(df_t['OI']) * np.log1p(np.abs(df_t['OI']))\n",
    "\n",
    "LIQ_SCALE = 1e12   # 3e-13 -> ~0.3\n",
    "VOL_SCALE = 100.0  # 0.01   -> ~1.0\n",
    "df_t['LIQ'] = df_t['LIQ'] * LIQ_SCALE\n",
    "df_t['VOL'] = df_t['VOL'] * VOL_SCALE\n",
    "\n",
    "df_t.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2201e7",
   "metadata": {},
   "source": [
    "## 3. Causal Graph (Fixed Topology)\n",
    "\n",
    "We use a **fixed DAG** across all dates:\n",
    "\n",
    "`MOM → HML`  \n",
    "`MOM → PC`  \n",
    "`HML → OI`  \n",
    "`OI → BAS`  \n",
    "`OI → PC`  \n",
    "`PC → BAS`  \n",
    "`BAS → LIQ`  \n",
    "`BAS → VOL`\n",
    "\n",
    "This topology encodes microstructure priors (order flow → prices/spreads; spreads → liquidity/volatility). Real markets have feedback; we treat this as a **working assumption** for tractable graph surgery.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dd58da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. features.csv ---\n",
    "features_long = df_t.reset_index().melt(\n",
    "    id_vars='Date', value_vars=ALL_NODES, var_name='node_id', value_name='value')\n",
    "features_long.rename(columns={'Date': 'date'}, inplace=True)\n",
    "features_long['date'] = pd.to_datetime(features_long['date']).dt.strftime(\"%Y-%m-%d\")\n",
    "features_path = OUTPUT_DIR / \"features.csv\"\n",
    "features_long.to_csv(features_path, index=False)\n",
    "print(f\"Saved features.csv to {features_path}\")\n",
    "\n",
    "# --- 2. nodes.csv ---\n",
    "nodes_df = pd.DataFrame({'node_id': ALL_NODES})\n",
    "nodes_path = OUTPUT_DIR / \"nodes.csv\"\n",
    "nodes_df.to_csv(nodes_path, index=False)\n",
    "print(f\"Saved nodes.csv to {nodes_path}\")\n",
    "\n",
    "# --- 3. edges.csv ---\n",
    "edges_list = [\n",
    "    {'source': 'Mom', 'target': 'HML'}, {'source': 'Mom', 'target': 'PC'},\n",
    "    {'source': 'HML', 'target': 'OI'},  {'source': 'OI', 'target': 'PC'},\n",
    "    {'source': 'OI', 'target': 'BAS'},  {'source': 'PC', 'target': 'BAS'},\n",
    "    {'source': 'BAS', 'target': 'LIQ'}, {'source': 'BAS', 'target': 'VOL'},\n",
    "]\n",
    "edges_df = pd.DataFrame(edges_list)\n",
    "edges_path = OUTPUT_DIR / \"edges.csv\"\n",
    "edges_df.to_csv(edges_path, index=False)\n",
    "print(f\"Saved edges.csv to {edges_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed0c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Visualizing the Constructed Causal Graph ---\")\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(nodes_df['node_id'])\n",
    "G.add_edges_from(edges_df.values)\n",
    "\n",
    "# We explicitly set the (x, y) coordinates for each node to ensure a clean,\n",
    "pos = {\n",
    "    'Mom': (0, 4),      # Top layer (Root Cause)\n",
    "    'HML': (-1.5, 3),   # Mid-layer 1\n",
    "    'PC':  (1.5, 3),\n",
    "    'OI':  (0, 2),      # Mid-layer 2\n",
    "    'BAS': (0, 1),      # Mid-layer 3\n",
    "    'LIQ': (-1, 0),     # Bottom layer (Outcomes)\n",
    "    'VOL': (1, 0)\n",
    "}\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "node_options = {\n",
    "    \"node_size\": 4500,\n",
    "    \"node_color\": \"#e0f7fa\",\n",
    "    \"edgecolors\": \"#0077b6\",\n",
    "    \"linewidths\": 2.0,\n",
    "}\n",
    "label_options = {\n",
    "    \"font_size\": 12,\n",
    "    \"font_weight\": \"bold\",\n",
    "}\n",
    "\n",
    "edge_options = {\n",
    "    \"width\": 2.0,\n",
    "    \"edge_color\": \"#495057\",\n",
    "    \"arrowstyle\": \"-|>\",  \n",
    "    \"arrowsize\": 25,\n",
    "}\n",
    "\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    **node_options,\n",
    "    font_size=label_options[\"font_size\"],\n",
    "    font_weight=label_options[\"font_weight\"],\n",
    "    **edge_options\n",
    ")\n",
    "\n",
    "plt.title(\"Hypothesized Causal Graph\", size=20)\n",
    "plt.axis('off') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e781eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Preprocessing Complete ---\")\n",
    "print(f\"Total nodes: {len(nodes_df)}\")\n",
    "print(f\"Total causal edges: {len(edges_df)}\")\n",
    "print(f\"Date range: {final_df_wide.index.min().date()} to {final_df_wide.index.max().date()}\")\n",
    "print(f\"Total time steps (days): {len(final_df_wide)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
